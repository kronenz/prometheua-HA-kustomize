# ğŸ“‹ 7ë…¸ë“œ 220í´ëŸ¬ìŠ¤í„° ë°°í¬ ìš”ì•½

> **Quick Reference**: ë…¸ë“œë³„ ë°°í¬ ëª…ë ¹ì–´ ë° ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

## ğŸ¯ ì•„í‚¤í…ì²˜ í•œëˆˆì— ë³´ê¸°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tier 1: Global Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Node 1     â”‚  â”‚   Node 2     â”‚            â”‚
â”‚  â”‚ Global Query â”‚  â”‚ Global HA    â”‚            â”‚
â”‚  â”‚ + Store      â”‚  â”‚ + Compactor  â”‚            â”‚
â”‚  â”‚ + Grafana    â”‚  â”‚ + Store      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”
â”‚ Node 3 â”‚   â”‚ Node 4 â”‚   â”‚ Node 5 â”‚   â”‚ Node 6 â”‚
â”‚Regionalâ”‚   â”‚Regionalâ”‚   â”‚Regionalâ”‚   â”‚Regionalâ”‚
â”‚  A1    â”‚   â”‚  A2    â”‚   â”‚  A3    â”‚   â”‚  BCD   â”‚
â”‚(1-60)  â”‚   â”‚(61-120)â”‚   â”‚(121-180)â”‚  â”‚(181-220)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚            â”‚            â”‚            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                 â”‚ Node 7  â”‚
                 â”‚MinIO S3 â”‚
                 â”‚ 10TB+   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ ë…¸ë“œë³„ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Node 1: Global + Store + Grafana

```bash
# 1. ë…¸ë“œ ì¤€ë¹„
kubectl label node node1 role=global tier=1

# 2. Namespace ìƒì„±
kubectl create namespace monitoring

# 3. S3 Secret ìƒì„±
kubectl create secret generic thanos-s3-config \
  --from-file=objstore.yml=/path/to/objstore.yml \
  -n monitoring

# 4. Global Query ë°°í¬
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: global-thanos-query
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: global-thanos-query
  template:
    metadata:
      labels:
        app: global-thanos-query
    spec:
      nodeSelector:
        role: global
      containers:
      - name: thanos-query
        image: quay.io/thanos/thanos:v0.37.2
        args:
        - query
        - --http-address=0.0.0.0:9090
        - --grpc-address=0.0.0.0:10901
        - --store=regional-query-a1.monitoring:10901
        - --store=regional-query-a2.monitoring:10901
        - --store=regional-query-a3.monitoring:10901
        - --store=regional-query-bcd.monitoring:10901
        - --store=dnssrv+_grpc._tcp.thanos-store.monitoring.svc.cluster.local
        resources:
          limits:
            cpu: 4000m
            memory: 8Gi
          requests:
            cpu: 2000m
            memory: 4Gi
        ports:
        - containerPort: 9090
        - containerPort: 10901
EOF

# 5. Store Gateway ë°°í¬
kubectl apply -f deploy/global/thanos-store-statefulset.yaml

# 6. Grafana ë°°í¬
helm install grafana grafana/grafana \
  --namespace monitoring \
  --set persistence.enabled=true \
  --set nodeSelector.role=global

# 7. ê²€ì¦
kubectl get pods -n monitoring -l app=global-thanos-query
kubectl logs -n monitoring -l app=global-thanos-query --tail=50
```

**âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Global Query Pod Running
- [ ] Store Gateway (2 replicas) Running
- [ ] Grafana Pod Running
- [ ] Global Queryì—ì„œ 4ê°œ Regional ì—°ê²° í™•ì¸

---

### Node 2: Global HA + Compactor

```bash
# 1. ë…¸ë“œ ì¤€ë¹„
kubectl label node node2 role=global-ha tier=1

# 2. Global Query Replica ë°°í¬
kubectl apply -f deploy/global/global-query-ha-deployment.yaml

# 3. Compactor ë°°í¬
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thanos-compactor
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: thanos-compactor
  template:
    metadata:
      labels:
        app: thanos-compactor
    spec:
      nodeSelector:
        role: global-ha
      containers:
      - name: thanos-compactor
        image: quay.io/thanos/thanos:v0.37.2
        args:
        - compact
        - --data-dir=/var/thanos/compactor
        - --objstore.config-file=/etc/thanos/objstore.yml
        - --retention.resolution-raw=7d
        - --retention.resolution-5m=30d
        - --retention.resolution-1h=90d
        - --compact.concurrency=3
        - --delete-delay=48h
        resources:
          limits:
            cpu: 4000m
            memory: 8Gi
          requests:
            cpu: 2000m
            memory: 4Gi
        volumeMounts:
        - name: objstore-config
          mountPath: /etc/thanos
        - name: data
          mountPath: /var/thanos/compactor
      volumes:
      - name: objstore-config
        secret:
          secretName: thanos-s3-config
      - name: data
        emptyDir: {}
EOF

# 4. Alertmanager ë°°í¬
kubectl apply -f deploy/global/alertmanager-statefulset.yaml

# 5. ê²€ì¦
kubectl get pods -n monitoring -l app=global-thanos-query
kubectl get pods -n monitoring -l app=thanos-compactor
kubectl logs -n monitoring -l app=thanos-compactor --tail=20 | grep "compact blocks"
```

**âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Global Query HA Pod Running
- [ ] Compactor Pod Running
- [ ] Alertmanager (3 replicas) Running
- [ ] Compaction ì‘ë™ í™•ì¸

---

### Node 3: Regional A1 (Cluster 1-60)

```bash
# 1. ë…¸ë“œ ì¤€ë¹„
kubectl label node node3 role=regional-a1 tier=2

# 2. Regional Query A1 ë°°í¬
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: regional-query-a1
  namespace: monitoring
spec:
  replicas: 2  # HA
  selector:
    matchLabels:
      app: regional-query-a1
  template:
    metadata:
      labels:
        app: regional-query-a1
        region: a1
    spec:
      nodeSelector:
        role: regional-a1
      containers:
      - name: thanos-query
        image: quay.io/thanos/thanos:v0.37.2
        args:
        - query
        - --http-address=0.0.0.0:9090
        - --grpc-address=0.0.0.0:10901
        - --query.replica-label=replica
        # Cluster 1-60 Sidecar ì—°ê²° (ë™ì  ë°œê²¬ ë˜ëŠ” ëª…ì‹œ)
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Service Discovery ì‚¬ìš©
        resources:
          limits:
            cpu: 4000m
            memory: 8Gi
          requests:
            cpu: 2000m
            memory: 4Gi
        ports:
        - containerPort: 9090
        - containerPort: 10901
EOF

# 3. Regional Store A1 ë°°í¬
kubectl apply -f deploy/regional-a1/regional-store-statefulset.yaml

# 4. ê²€ì¦
kubectl get pods -n monitoring -l app=regional-query-a1
kubectl logs -n monitoring -l app=regional-query-a1 --tail=20
```

**âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Regional Query A1 (2 replicas) Running
- [ ] Regional Store A1 Running
- [ ] gRPC ì—°ê²° ìˆ˜ í™•ì¸ (60ê°œ ì˜ˆìƒ)

---

### Node 4: Regional A2 (Cluster 61-120)

```bash
# 1. ë…¸ë“œ ì¤€ë¹„
kubectl label node node4 role=regional-a2 tier=2

# 2. Regional Query A2 ë°°í¬
kubectl apply -f deploy/regional-a2/regional-query-deployment.yaml

# 3. Regional Store A2 ë°°í¬
kubectl apply -f deploy/regional-a2/regional-store-statefulset.yaml

# 4. ê²€ì¦
kubectl get pods -n monitoring -l app=regional-query-a2
```

**âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Regional Query A2 (2 replicas) Running
- [ ] Regional Store A2 Running

---

### Node 5: Regional A3 (Cluster 121-180)

```bash
# 1. ë…¸ë“œ ì¤€ë¹„
kubectl label node node5 role=regional-a3 tier=2

# 2. Regional Query A3 ë°°í¬
kubectl apply -f deploy/regional-a3/regional-query-deployment.yaml

# 3. Regional Store A3 ë°°í¬
kubectl apply -f deploy/regional-a3/regional-store-statefulset.yaml

# 4. ê²€ì¦
kubectl get pods -n monitoring -l app=regional-query-a3
```

**âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Regional Query A3 (2 replicas) Running
- [ ] Regional Store A3 Running

---

### Node 6: Regional BCD (Cluster 181-220)

```bash
# 1. ë…¸ë“œ ì¤€ë¹„
kubectl label node node6 role=regional-bcd tier=2

# 2. Regional Query BCD ë°°í¬
kubectl apply -f deploy/regional-bcd/regional-query-deployment.yaml

# 3. Regional Store BCD ë°°í¬
kubectl apply -f deploy/regional-bcd/regional-store-statefulset.yaml

# 4. ê²€ì¦
kubectl get pods -n monitoring -l app=regional-query-bcd
```

**âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Regional Query BCD (2 replicas) Running
- [ ] Regional Store BCD Running
- [ ] gRPC ì—°ê²° ìˆ˜ í™•ì¸ (40ê°œ ì˜ˆìƒ)

---

### Node 7: MinIO S3

```bash
# 1. ë…¸ë“œ ì¤€ë¹„
kubectl label node node7 role=storage tier=3

# 2. ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ ë§ˆìš´íŠ¸
ssh node7
sudo mkdir -p /mnt/minio-data
# NFS/iSCSI ë“± ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ ë§ˆìš´íŠ¸
sudo mount /dev/sdb1 /mnt/minio-data  # ì˜ˆì‹œ

# 3. MinIO ë°°í¬
kubectl create namespace storage

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
  namespace: storage
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      nodeSelector:
        role: storage
      containers:
      - name: minio
        image: minio/minio:latest
        args:
        - server
        - /data
        - --console-address
        - ":9001"
        env:
        - name: MINIO_ROOT_USER
          value: "minio"
        - name: MINIO_ROOT_PASSWORD
          value: "minio123"
        resources:
          limits:
            cpu: 8000m
            memory: 16Gi
          requests:
            cpu: 4000m
            memory: 8Gi
        volumeMounts:
        - name: data
          mountPath: /data
        ports:
        - containerPort: 9000
        - containerPort: 9001
      volumes:
      - name: data
        hostPath:
          path: /mnt/minio-data
          type: Directory
---
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: storage
spec:
  type: ClusterIP
  ports:
  - name: api
    port: 9000
    targetPort: 9000
  - name: console
    port: 9001
    targetPort: 9001
  selector:
    app: minio
EOF

# 4. S3 ë²„í‚· ìƒì„±
kubectl run -it --rm mc --image=minio/mc --restart=Never -- \
  /bin/sh -c "mc alias set minio http://minio.storage:9000 minio minio123 && \
              mc mb minio/thanos-bucket"

# 5. ê²€ì¦
kubectl get pods -n storage -l app=minio
kubectl exec -n storage -it minio-xxx -- ls -lh /data
```

**âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] MinIO Pod Running
- [ ] MinIO Console ì ‘ê·¼ ê°€ëŠ¥ (http://minio.storage:9001)
- [ ] thanos-bucket ìƒì„± í™•ì¸
- [ ] ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ ë§ˆìš´íŠ¸ í™•ì¸

---

## ğŸ” ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦

### 1. Pod ìƒíƒœ í™•ì¸

```bash
# ëª¨ë“  ëª¨ë‹ˆí„°ë§ Pod í™•ì¸
kubectl get pods -n monitoring -o wide

# ì˜ˆìƒ ì¶œë ¥:
# global-thanos-query-xxx          1/1  Running  node1
# global-thanos-query-ha-xxx       1/1  Running  node2
# thanos-store-0                   1/1  Running  node1
# thanos-store-1                   1/1  Running  node1
# thanos-store-2                   1/1  Running  node2
# thanos-compactor-xxx             1/1  Running  node2
# grafana-xxx                      1/1  Running  node1
# alertmanager-0,1,2               1/1  Running  node2
# regional-query-a1-xxx (2ê°œ)      1/1  Running  node3
# regional-query-a2-xxx (2ê°œ)      1/1  Running  node4
# regional-query-a3-xxx (2ê°œ)      1/1  Running  node5
# regional-query-bcd-xxx (2ê°œ)     1/1  Running  node6
# regional-store-a1-0              1/1  Running  node3
# regional-store-a2-0              1/1  Running  node4
# regional-store-a3-0              1/1  Running  node5
# regional-store-bcd-0             1/1  Running  node6
```

### 2. Store ì—°ê²° í™•ì¸

```bash
# Global Queryì—ì„œ ëª¨ë“  Store í™•ì¸
kubectl exec -n monitoring global-thanos-query-xxx -- \
  wget -qO- http://localhost:9090/api/v1/stores | jq '.data[] | {name, lastCheck, lastError}'

# ì˜ˆìƒ ì¶œë ¥: ì´ 11ê°œ ì—°ê²°
# - regional-query-a1:10901    (Cluster 1-60)
# - regional-query-a2:10901    (Cluster 61-120)
# - regional-query-a3:10901    (Cluster 121-180)
# - regional-query-bcd:10901   (Cluster 181-220)
# - thanos-store-0:10901       (S3 ê³¼ê±° ë°ì´í„°)
# - thanos-store-1:10901
# - thanos-store-2:10901
# - regional-store-a1-0:10901
# - regional-store-a2-0:10901
# - regional-store-a3-0:10901
# - regional-store-bcd-0:10901
```

### 3. S3 ì—…ë¡œë“œ í™•ì¸

```bash
# MinIO ë²„í‚· í™•ì¸
kubectl exec -n storage minio-xxx -- \
  mc ls minio/thanos-bucket

# ì˜ˆìƒ: 220ê°œ í´ëŸ¬ìŠ¤í„°ì˜ 2ì‹œê°„ ë¸”ë¡
```

### 4. Grafana ì ‘ì†

```bash
# Grafana í¬íŠ¸ í¬ì›Œë”©
kubectl port-forward -n monitoring svc/grafana 3000:3000

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:3000 ì ‘ì†
# ID: admin
# PW: kubectl get secret -n monitoring grafana -o jsonpath="{.data.admin-password}" | base64 -d

# Datasource í™•ì¸: Thanos-Query (http://global-thanos-query:9090)
```

### 5. ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸

```promql
# ì „ì²´ í´ëŸ¬ìŠ¤í„° Up í™•ì¸
sum(up) by (cluster)
# ì˜ˆìƒ: 220ê°œ í´ëŸ¬ìŠ¤í„° ê²°ê³¼

# Regionalë³„ CPU ì‚¬ìš©ë¥ 
sum(rate(container_cpu_usage_seconds_total[5m])) by (region)

# Global ë©”íŠ¸ë¦­
count(up == 1)
# ì˜ˆìƒ: ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œ íƒ€ê²Ÿ
```

---

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: Regional Queryê°€ Sidecar ì—°ê²° ëª»í•¨

**ì¦ìƒ:**
```
kubectl logs -n monitoring regional-query-a1-xxx
error: no store found
```

**í•´ê²°:**
1. Sidecar Service í™•ì¸
2. NetworkPolicy í™•ì¸
3. DNS í™•ì¸

```bash
# ê° App í´ëŸ¬ìŠ¤í„°ì—ì„œ
kubectl get svc -n monitoring | grep sidecar
kubectl get networkpolicy -n monitoring
```

### ë¬¸ì œ 2: Global Query ì‘ë‹µ ëŠë¦¼

**ì¦ìƒ:** ì¿¼ë¦¬ ì‘ë‹µ ì‹œê°„ > 30ì´ˆ

**í•´ê²°:**
1. Regional Query ë¶€í•˜ í™•ì¸
2. Recording Rules ì¶”ê°€
3. Query ìºì‹± í™œì„±í™”

```bash
# Regional Query ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl top pods -n monitoring -l app=regional-query-a1

# Query ë©”íŠ¸ë¦­ í™•ì¸
kubectl exec -n monitoring global-thanos-query-xxx -- \
  wget -qO- http://localhost:9090/metrics | grep thanos_query_duration
```

### ë¬¸ì œ 3: S3 ì—…ë¡œë“œ ì‹¤íŒ¨

**ì¦ìƒ:** Sidecarì—ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨ ë¡œê·¸

**í•´ê²°:**
1. MinIO ì ‘ê·¼ í™•ì¸
2. S3 Secret í™•ì¸
3. ë„¤íŠ¸ì›Œí¬ í™•ì¸

```bash
# MinIO ìƒíƒœ í™•ì¸
kubectl get pods -n storage -l app=minio

# S3 Secret í™•ì¸
kubectl get secret -n monitoring thanos-s3-config -o yaml

# ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
kubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- \
  curl http://minio.storage:9000/minio/health/live
```

---

## ğŸ“Š ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§

### ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 

```bash
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 5 'kubectl top nodes | grep -E "node[1-7]"'
```

### Pod ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 

```bash
# Top 10 CPU ì‚¬ìš© Pod
kubectl top pods -n monitoring --sort-by=cpu | head -11

# Top 10 Memory ì‚¬ìš© Pod
kubectl top pods -n monitoring --sort-by=memory | head -11
```

### ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­

```bash
# ê° ë…¸ë“œì—ì„œ
for i in {1..7}; do
  echo "=== Node $i ==="
  ssh node$i "sar -n DEV 1 1 | grep -E 'eth0|ens'"
done
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Recording Rules êµ¬ì„± (ê³ ë¹ˆë„ ì¿¼ë¦¬ ì‚¬ì „ ê³„ì‚°)
- [ ] Query ê²°ê³¼ ìºì‹± í™œì„±í™”
- [ ] S3 Compaction ì£¼ê¸° ìµœì í™” (3ë¶„ â†’ 5ë¶„)
- [ ] Regional Query ë¦¬ì†ŒìŠ¤ ì¦ì„¤ (í•„ìš” ì‹œ)
- [ ] NetworkPolicyë¡œ ë¶ˆí•„ìš”í•œ íŠ¸ë˜í”½ ì°¨ë‹¨
- [ ] Grafana ëŒ€ì‹œë³´ë“œ ìµœì í™” (ì¿¼ë¦¬ ë‹¨ìˆœí™”)

---

**ë°°í¬ ì™„ë£Œ í›„ ë‹¤ìŒ ë‹¨ê³„:**
1. [OPERATIONS.md](./OPERATIONS.md) - ì¼ìƒ ìš´ì˜ ê°€ì´ë“œ
2. [BEST_PRACTICES.md](./BEST_PRACTICES.md) - ì„±ëŠ¥ ìµœì í™”
3. [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) - ë¬¸ì œ í•´ê²°

**Last Updated**: 2025-10-15
**Architecture**: 7 Nodes, 220 Clusters, 3-Tier Hierarchical

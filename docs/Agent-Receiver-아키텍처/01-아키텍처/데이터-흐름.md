# ë°ì´í„° íë¦„

## ğŸ“‹ ê°œìš”

Prometheus Agentì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ë¶€í„° Grafana ì‹œê°í™”ê¹Œì§€ ì „ì²´ ë°ì´í„° íë¦„ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

### ë°ì´í„° ì—¬ì •: 15ì´ˆì—ì„œ ì˜ì›ê¹Œì§€

í•˜ë‚˜ì˜ ë©”íŠ¸ë¦­ì´ Kubernetes í´ëŸ¬ìŠ¤í„°ì—ì„œ ìƒì„±ë˜ì–´ ì¥ê¸° ì €ì¥ì†Œì— ì•ˆì°©í•˜ê³ , ëª‡ ê°œì›” í›„ ëŒ€ì‹œë³´ë“œì— í‘œì‹œë˜ê¸°ê¹Œì§€ì˜ ì—¬ì •ì€ **7ë‹¨ê³„ì˜ ì •êµí•œ íŒŒì´í”„ë¼ì¸**ì„ ê±°ì¹©ë‹ˆë‹¤.

**ì‹¤ì œ ë©”íŠ¸ë¦­ í•˜ë‚˜ì˜ ìƒì• **:
```
[10:00:00] Pod CPU ì‚¬ìš©ë¥  ì¸¡ì •: 45%
[10:00:15] Prometheus Agentê°€ Scrape â†’ WAL ê¸°ë¡
[10:00:25] Remote Write Queue ì ì¬ (ë°°ì¹˜ 5000ê°œ ì¤‘ í•˜ë‚˜)
[10:00:30] Thanos Receiver ì „ì†¡ â†’ Hashringìœ¼ë¡œ Receiver-1 ì„ íƒ
[10:00:31] Replication Factor=3: Receiver-1, 2, 3ì— ë³‘ë ¬ ì €ì¥
[12:00:00] 2ì‹œê°„ TSDB ë¸”ë¡ ì™„ì„± â†’ S3 ì—…ë¡œë“œ
[12:30:00] Compactorê°€ Downsampling: Raw(15s) â†’ 5m â†’ 1h
[90ì¼ í›„] Grafanaì—ì„œ "ì§€ë‚œ 3ê°œì›” CPU ì¶”ì„¸" ì¿¼ë¦¬ â†’ 1h ë‹¤ìš´ìƒ˜í”Œ ë¸”ë¡ì—ì„œ ì¡°íšŒ (3.2ì´ˆ)
```

ì´ ë¬¸ì„œì—ì„œëŠ” ê° ë‹¨ê³„ì˜ **ê¸°ìˆ ì  êµ¬í˜„**, **ë³‘ëª© ì§€ì **, **ì‹¤ì „ ìµœì í™” ì‚¬ë¡€**ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ”„ C4 Dynamic Diagram (ë°ì´í„° íë¦„)

```mermaid
C4Dynamic
    title Dynamic Diagram - Metrics Data Flow (Write Path)

    ContainerDb(agent, "Prometheus Agent", "Agent Mode", "Cluster-02/03/04")
    Container(wal, "WAL", "Write-Ahead Log", "Local PVC")
    Container(queue, "Remote Write Queue", "In-Memory Queue", "Capacity: 20000")

    Container(ingress, "Nginx Ingress", "HTTP Router", "Central Cluster")
    ContainerDb(receiver, "Thanos Receiver", "StatefulSet", "Hashring + TSDB")
    Container(replication, "Replication", "gRPC", "RF=3")
    ContainerDb(tsdb, "TSDB", "Time Series DB", "2h blocks")
    ContainerDb(s3, "MinIO S3", "Object Storage", "Long-term Storage")

    Rel(agent, wal, "1. Scrape & Write", "15s interval")
    Rel(wal, queue, "2. Enqueue Samples", "Continuous")
    Rel(queue, ingress, "3. Remote Write", "HTTPS POST<br/>Batch: 5000 samples")
    Rel(ingress, receiver, "4. Route to Hashring", "HTTP/19291")
    Rel(receiver, replication, "5. Replicate", "gRPC to 2 peers")
    Rel(receiver, tsdb, "6. Write Local TSDB", "Samples insert")
    Rel(tsdb, s3, "7. Upload Block", "Every 2h<br/>S3 PUT")

    UpdateRelStyle(agent, wal, $offsetY="-40", $offsetX="-50")
    UpdateRelStyle(wal, queue, $offsetY="-30")
    UpdateRelStyle(queue, ingress, $offsetY="-40")
    UpdateRelStyle(ingress, receiver, $offsetY="-30")
    UpdateRelStyle(receiver, replication, $offsetX="50")
    UpdateRelStyle(receiver, tsdb, $offsetY="-30")
    UpdateRelStyle(tsdb, s3, $offsetY="-40")
```

---

## ğŸ”„ C4 Dynamic Diagram (ì¿¼ë¦¬ íë¦„)

```mermaid
C4Dynamic
    title Dynamic Diagram - Metrics Query Flow (Read Path)

    Container(grafana, "Grafana", "Dashboard", "User Interface")
    Container(query, "Thanos Query", "Query Engine", "PromQL + Deduplication")
    ContainerDb(receiver, "Thanos Receiver", "StatefulSet", "Recent metrics <2h")
    ContainerDb(store, "Thanos Store", "Gateway", "Historical metrics >2h")
    ContainerDb(s3, "MinIO S3", "Object Storage", "TSDB Blocks")
    Container(compactor, "Thanos Compactor", "Downsampler", "5m, 1h resolution")

    Rel(grafana, query, "1. PromQL Query", "HTTP/9090<br/>rate(cpu[5m])")
    Rel(query, receiver, "2a. Query Recent", "gRPC StoreAPI<br/>Last 2 hours")
    Rel(query, store, "2b. Query Historical", "gRPC StoreAPI<br/>Older than 2h")
    Rel(store, s3, "3. Fetch Blocks", "S3 GET<br/>Read TSDB blocks")
    Rel(compactor, s3, "Background: Downsample", "5m/1h aggregation")
    Rel(s3, store, "4. Return Data", "TSDB blocks")
    Rel(receiver, query, "5a. Return Recent", "Time series")
    Rel(store, query, "5b. Return Historical", "Time series")
    Rel(query, query, "6. Deduplicate", "replica label")
    Rel(query, grafana, "7. Result", "JSON response")

    UpdateRelStyle(grafana, query, $offsetY="-40")
    UpdateRelStyle(query, receiver, $offsetX="-80", $offsetY="-20")
    UpdateRelStyle(query, store, $offsetX="80", $offsetY="-20")
    UpdateRelStyle(store, s3, $offsetY="-30")
```

---

## ğŸ”„ ì „ì²´ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant Target as Target<br/>(Pod/Node)
    participant Agent as Prometheus Agent<br/>(Edge)
    participant WAL as WAL<br/>(Local Disk)
    participant Queue as Remote Write Queue
    participant Ingress as Nginx Ingress<br/>(Central)
    participant Receiver as Thanos Receiver
    participant TSDB as Receiver TSDB
    participant S3 as MinIO S3
    participant Query as Thanos Query
    participant Store as Thanos Store
    participant Grafana as Grafana

    Note over Target,Agent: 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (15s interval)
    Target->>Agent: /metrics endpoint scrape
    Agent->>WAL: Write to WAL

    Note over WAL,Queue: 2. Remote Write ì¤€ë¹„
    WAL->>Queue: Samples to Queue
    Queue->>Queue: Batch samples<br/>(10s deadline)

    Note over Queue,Receiver: 3. ë„¤íŠ¸ì›Œí¬ ì „ì†¡
    Queue->>Ingress: HTTPS POST<br/>/api/v1/receive
    Ingress->>Receiver: Forward request
    Receiver->>Receiver: Hashring distribution
    Receiver->>TSDB: Write to TSDB

    Note over TSDB,S3: 4. ì¥ê¸° ì €ì¥ (2h ë¸”ë¡)
    TSDB->>TSDB: Create 2h block
    TSDB->>S3: Upload block

    Note over Query,Grafana: 5. ì¿¼ë¦¬ ë° ì‹œê°í™”
    Grafana->>Query: PromQL query
    Query->>TSDB: Recent data (<2h)
    Query->>Store: Historical data (>2h)
    Store->>S3: Fetch blocks
    S3-->>Store: Return blocks
    Store-->>Query: Return metrics
    TSDB-->>Query: Return metrics
    Query-->>Grafana: Aggregated result
    Grafana->>Grafana: Render dashboard
```

---

## 1ï¸âƒ£ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Edge Cluster)

### Scrape í”„ë¡œì„¸ìŠ¤

```mermaid
graph LR
    SD[Service Discovery] --> TARGETS[Target List]
    TARGETS --> SCRAPE[Scrape Job]
    SCRAPE --> NODE[Node Exporter<br/>:9100/metrics]
    SCRAPE --> KSM[Kube-State-Metrics<br/>:8080/metrics]
    SCRAPE --> KUBELET[Kubelet<br/>:10250/metrics]

    NODE --> PARSE[Parse Metrics]
    KSM --> PARSE
    KUBELET --> PARSE

    PARSE --> LABELS[Add Labels<br/>cluster, namespace, pod]
    LABELS --> WAL[Write-Ahead Log]
```

### Scrape ì„¤ì • ì˜ˆì‹œ

```yaml
# ServiceMonitor (Prometheus Operator)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: node
    - targetLabel: cluster
      replacement: cluster-02
```

### ìˆ˜ì§‘ë˜ëŠ” ë©”íŠ¸ë¦­ ì˜ˆì‹œ

```prometheus
# Node Exporter
node_cpu_seconds_total{cluster="cluster-02",cpu="0",mode="idle"} 12345.67
node_memory_MemAvailable_bytes{cluster="cluster-02"} 4294967296

# Kube-State-Metrics
kube_pod_status_phase{cluster="cluster-02",namespace="default",pod="nginx-1",phase="Running"} 1

# Kubelet
kubelet_running_pods{cluster="cluster-02",instance="node-1"} 25
```

---

## 2ï¸âƒ£ WAL (Write-Ahead Log)

### WAL ì—­í• 

1. **ë²„í¼ë§**: Remote Write ì‹¤íŒ¨ ì‹œ ë°ì´í„° ë³´ì¡´
2. **ì¬ì „ì†¡**: ë„¤íŠ¸ì›Œí¬ ë³µêµ¬ í›„ ìë™ ì¬ì „ì†¡
3. **ì„±ëŠ¥**: ë””ìŠ¤í¬ I/Oë¥¼ ìˆœì°¨ ì“°ê¸°ë¡œ ìµœì í™”

### WAL êµ¬ì¡°

```
/data/wal/
â”œâ”€â”€ 00000000  # WAL segment 0
â”œâ”€â”€ 00000001  # WAL segment 1
â”œâ”€â”€ 00000002  # WAL segment 2
â””â”€â”€ checkpoint.00000001  # ì²´í¬í¬ì¸íŠ¸
```

### WAL í”Œë¡œìš°

```mermaid
graph TB
    SCRAPE[Scrape Samples] --> WAL_WRITE[Write to WAL]
    WAL_WRITE --> SEGMENT[WAL Segment<br/>128MB]

    SEGMENT --> CHECK{Segment Full?}
    CHECK -->|Yes| NEW_SEG[Create New Segment]
    CHECK -->|No| WAIT[Continue Writing]

    WAL_WRITE --> QUEUE[Remote Write Queue]

    QUEUE --> SUCCESS{Send Success?}
    SUCCESS -->|Yes| DELETE[Delete from WAL<br/>after retention]
    SUCCESS -->|No| RETRY[Retry from WAL]
```

### WAL ì„¤ì •

```yaml
server:
  extraArgs:
    storage.agent.path: /data
    storage.agent.wal-compression: true  # WAL ì••ì¶•
    storage.agent.retention.max-time: 4h  # ìµœëŒ€ 4ì‹œê°„ ë³´ì¡´
    storage.agent.retention.min-time: 1h  # ìµœì†Œ 1ì‹œê°„ ë³´ì¡´
```

---

## 3ï¸âƒ£ Remote Write Queue

### Queue ë™ì‘ ì›ë¦¬

```mermaid
graph TB
    WAL[WAL] --> QUEUE[Queue<br/>Capacity: 20000]

    QUEUE --> SHARD1[Shard 1<br/>10 samples/batch]
    QUEUE --> SHARD2[Shard 2<br/>10 samples/batch]
    QUEUE --> SHARDN[Shard N<br/>10 samples/batch]

    SHARD1 --> BATCH1[Batch 1<br/>10s deadline]
    SHARD2 --> BATCH2[Batch 2<br/>10s deadline]
    SHARDN --> BATCHN[Batch N<br/>10s deadline]

    BATCH1 --> SEND[Send to Receiver]
    BATCH2 --> SEND
    BATCHN --> SEND

    SEND --> SUCCESS{Success?}
    SUCCESS -->|Yes| ACK[ACK to WAL]
    SUCCESS -->|No| BACKOFF[Exponential Backoff]
    BACKOFF --> RETRY[Retry]
```

### Queue ë©”íŠ¸ë¦­

```promql
# í ê¸¸ì´
prometheus_remote_storage_queue_length

# í ìš©ëŸ‰
prometheus_remote_storage_queue_capacity

# Shard ìˆ˜
prometheus_remote_storage_shards

# ì „ì†¡ ì„±ê³µ/ì‹¤íŒ¨
rate(prometheus_remote_storage_succeeded_samples_total[5m])
rate(prometheus_remote_storage_failed_samples_total[5m])
```

---

## 4ï¸âƒ£ ë„¤íŠ¸ì›Œí¬ ì „ì†¡ (HTTPS)

### Remote Write ìš”ì²­ êµ¬ì¡°

```http
POST /api/v1/receive HTTP/1.1
Host: thanos-receive.monitoring.svc.cluster.local:19291
Content-Type: application/x-protobuf
Content-Encoding: snappy
X-Prometheus-Remote-Write-Version: 0.1.0

[Protocol Buffer Payload - Snappy Compressed]
```

### Protocol Buffer êµ¬ì¡°

```protobuf
message WriteRequest {
  repeated TimeSeries timeseries = 1;
}

message TimeSeries {
  repeated Label labels = 1;
  repeated Sample samples = 2;
}

message Label {
  string name = 1;
  string value = 2;
}

message Sample {
  double value = 1;
  int64 timestamp = 2;
}
```

### ì••ì¶• íš¨ê³¼

```
ì›ë³¸ í¬ê¸°: 1,000,000 samples Ã— 16 bytes = 16MB
Snappy ì••ì¶• í›„: ~7MB (ì•½ 56% ì••ì¶•)
```

---

## 5ï¸âƒ£ Thanos Receiver ì²˜ë¦¬

### Receiver ë‚´ë¶€ í”Œë¡œìš°

```mermaid
graph TB
    HTTP[HTTP Request<br/>/api/v1/receive] --> DECOMPRESS[Snappy Decompress]
    DECOMPRESS --> PARSE[Parse Proto]
    PARSE --> HASHRING[Hashring<br/>Consistent Hash]

    HASHRING --> RECV0{Receiver-0?}
    HASHRING --> RECV1{Receiver-1?}
    HASHRING --> RECV2{Receiver-2?}

    RECV0 -->|Yes| TSDB0[Local TSDB-0]
    RECV0 -->|Replicate| TSDB1[Local TSDB-1]
    RECV0 -->|Replicate| TSDB2[Local TSDB-2]

    TSDB0 --> HEAD[Head Block<br/>in-memory]
    HEAD --> PERSIST[Persist to Disk<br/>2h block]
```

### Hashring ë¶„ë°°

```json
// ì‹œê³„ì—´: {__name__="cpu_usage", cluster="cluster-02", pod="nginx-1"}
// Hash(cluster-02/nginx-1) = 0x4a3c2f1e

Hashring:
  Receiver-0: 0x00000000 - 0x55555555
  Receiver-1: 0x55555556 - 0xaaaaaaaa
  Receiver-2: 0xaaaaaaab - 0xffffffff

â†’ 0x4a3c2f1eëŠ” Receiver-0 ë²”ìœ„ â†’ Receiver-0ìœ¼ë¡œ ë¼ìš°íŒ…
â†’ Replication Factor=3 â†’ Receiver-1, Receiver-2ì—ë„ ë³µì œ
```

### TSDB ì“°ê¸°

```mermaid
sequenceDiagram
    participant Receiver
    participant Head as Head Block<br/>(Memory)
    participant WAL as Receiver WAL
    participant Disk as 2h Block<br/>(Disk)

    Receiver->>WAL: Write to WAL (durability)
    Receiver->>Head: Write to Head (fast query)

    Note over Head: 2ì‹œê°„ í›„
    Head->>Disk: Persist as 2h block
    Disk->>Disk: Compress (TSDB format)
```

---

## 6ï¸âƒ£ S3 ì—…ë¡œë“œ (ì¥ê¸° ì €ì¥)

### ë¸”ë¡ ì—…ë¡œë“œ í”Œë¡œìš°

```mermaid
graph TB
    TSDB[Local TSDB] --> BLOCK[2h Block Complete]
    BLOCK --> UPLOAD[Upload to S3]

    UPLOAD --> S3[MinIO S3<br/>Bucket: thanos-cluster-01]

    S3 --> META[Block Metadata<br/>meta.json]
    S3 --> INDEX[Block Index<br/>index]
    S3 --> CHUNKS[Chunks<br/>chunks/000001]

    UPLOAD --> VERIFY{Upload Success?}
    VERIFY -->|Yes| DELETE[Delete Local Block<br/>after retention]
    VERIFY -->|No| RETRY[Retry Upload]
```

### S3 ë¸”ë¡ êµ¬ì¡°

```
s3://thanos-cluster-01/
â”œâ”€â”€ 01H9XYZABC123/          # Block ID (ULID)
â”‚   â”œâ”€â”€ meta.json           # ë©”íƒ€ë°ì´í„° (ì‹œê°„ ë²”ìœ„, í†µê³„)
â”‚   â”œâ”€â”€ index               # ì‹œê³„ì—´ ì¸ë±ìŠ¤
â”‚   â””â”€â”€ chunks/
â”‚       â”œâ”€â”€ 000001          # ì••ì¶•ëœ ìƒ˜í”Œ ë°ì´í„°
â”‚       â”œâ”€â”€ 000002
â”‚       â””â”€â”€ ...
â””â”€â”€ 01H9XYZ123DEF/
    â”œâ”€â”€ meta.json
    â”œâ”€â”€ index
    â””â”€â”€ chunks/
```

### meta.json ì˜ˆì‹œ

```json
{
  "version": 1,
  "ulid": "01H9XYZABC123",
  "minTime": 1697097600000,
  "maxTime": 1697104800000,
  "stats": {
    "numSamples": 1500000,
    "numSeries": 5000,
    "numChunks": 15000
  },
  "compaction": {
    "level": 1,
    "sources": ["01H9XYZABC123"]
  },
  "thanos": {
    "labels": {
      "cluster": "cluster-01",
      "receive": "true"
    }
  }
}
```

---

## 7ï¸âƒ£ ì¿¼ë¦¬ ë° ì¡°íšŒ

### ì¿¼ë¦¬ ê²½ë¡œ ì„ íƒ

```mermaid
graph TB
    GRAFANA[Grafana Query<br/>Last 24h] --> QUERY[Thanos Query]

    QUERY --> SPLIT{Time Range Split}

    SPLIT -->|Recent<br/>< 2h| RECEIVER[Thanos Receiver<br/>In-Memory Head]
    SPLIT -->|Recent<br/>2h - 15d| PROM[Prometheus HA<br/>Local TSDB]
    SPLIT -->|Historical<br/>> 15d| STORE[Thanos Store<br/>S3 Blocks]

    RECEIVER --> RESULT[Merge Results]
    PROM --> RESULT
    STORE --> RESULT

    RESULT --> DEDUP[Deduplication<br/>replica label]
    DEDUP --> GRAFANA
```

### ì¿¼ë¦¬ ì˜ˆì‹œ

```promql
# Grafanaì—ì„œ ì‹¤í–‰ëœ ì¿¼ë¦¬
rate(container_cpu_usage_seconds_total{cluster="cluster-02"}[5m])

# Thanos Query ì²˜ë¦¬
1. Receiver (ìµœê·¼ 2h): 2025-10-20 10:00 ~ 12:00
2. Prometheus HA (2h ~ 15d): 2025-10-05 ~ 2025-10-20 10:00
3. Store (> 15d): S3ì—ì„œ ì¡°íšŒ
4. Merge + Deduplicate
5. Return to Grafana
```

### Store Gateway ì¡°íšŒ

```mermaid
sequenceDiagram
    participant Query as Thanos Query
    participant Store as Thanos Store
    participant Cache as Index Cache<br/>(Memcached)
    participant S3 as MinIO S3

    Query->>Store: Query range 2025-10-10 ~ 2025-10-15
    Store->>Cache: Check index cache

    alt Cache Hit
        Cache-->>Store: Return index
    else Cache Miss
        Store->>S3: Fetch block index
        S3-->>Store: Return index
        Store->>Cache: Store in cache
    end

    Store->>S3: Fetch chunk data
    S3-->>Store: Return chunks
    Store->>Store: Decompress & Filter
    Store-->>Query: Return samples
```

---

## ğŸ“Š ë°ì´í„° íë¦„ ì„±ëŠ¥ ë©”íŠ¸ë¦­

### End-to-End ë ˆì´í„´ì‹œ

```
1. Scrape â†’ WAL: ~50ms
2. WAL â†’ Remote Write Queue: ~10ms
3. Queue â†’ Network Send: ~10s (batch deadline)
4. Network â†’ Receiver: ~100ms
5. Receiver â†’ TSDB Write: ~50ms
6. TSDB â†’ S3 Upload: ~2h (block boundary)

ì‹¤ì‹œê°„ ì¿¼ë¦¬ (Grafana â†’ Receiver):
7. Query â†’ Receiver: ~100ms
8. Receiver â†’ Return: ~50ms

Total (Scrape â†’ Queryable): ~10-11ì´ˆ
```

### ì²˜ë¦¬ëŸ‰ (Throughput)

```
Edge Cluster (cluster-02):
- Scrape targets: 100ê°œ
- Metrics/target: 1000ê°œ
- Scrape interval: 15s

â†’ Samples/sec = 100 Ã— 1000 / 15 = 6,666 samples/sec
â†’ Remote Write bandwidth = 6,666 Ã— 16 bytes Ã— 0.5 (ì••ì¶•) = 53KB/s

Central Cluster (4 edge clusters):
â†’ Total samples/sec = 6,666 Ã— 4 = 26,664 samples/sec
â†’ Total bandwidth = 53KB/s Ã— 4 = 212KB/s
```

---

## ğŸš¨ ë°ì´í„° ì†ì‹¤ ë°©ì§€ ë©”ì»¤ë‹ˆì¦˜

### 1. WAL ë‚´êµ¬ì„±
```yaml
# Agentê°€ ì¬ì‹œì‘ë˜ì–´ë„ WALì—ì„œ ë³µêµ¬
- Pod Crash â†’ WAL ìœ ì§€ (PVC)
- Remote Write ì¬ê°œ â†’ WALì—ì„œ ì „ì†¡
```

### 2. Remote Write ì¬ì „ì†¡
```yaml
# ë„¤íŠ¸ì›Œí¬ ì¥ì•  ì‹œ ìë™ ì¬ì‹œë„
- Exponential Backoff (30ms â†’ 5s)
- ìµœëŒ€ 4ì‹œê°„ WAL ë²„í¼
```

### 3. Receiver Replication
```yaml
# Receiver ì¥ì•  ì‹œ ë³µì œë³¸ ì‚¬ìš©
- Replication Factor: 3
- Receiver-0 down â†’ Receiver-1, 2ì— ë°ì´í„° ì¡´ì¬
```

### 4. S3 Durability
```yaml
# MinIO Erasure Coding (EC:4)
- 4ê°œ ë…¸ë“œ ì¤‘ 2ê°œ ì†ì‹¤ê¹Œì§€ ë³µêµ¬ ê°€ëŠ¥
- 99.999999999% (11 nines) durability
```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜** â†’ [ì „ì²´-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜.md](./ì „ì²´-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜.md)
- **Remote Write ìµœì í™”** â†’ [../09-ì„±ëŠ¥-ìµœì í™”/Remote-Write-ìµœì í™”.md](../09-ì„±ëŠ¥-ìµœì í™”/Remote-Write-ìµœì í™”.md)
- **Thanos Receiver íŒ¨í„´** â†’ [Thanos-Receiver-íŒ¨í„´.md](./Thanos-Receiver-íŒ¨í„´.md)

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-20

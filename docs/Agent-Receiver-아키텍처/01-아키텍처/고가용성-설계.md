# ê³ ê°€ìš©ì„± (HA) ì„¤ê³„

## ğŸ“‹ ê°œìš”

4ê°œ í´ëŸ¬ìŠ¤í„° ë©€í‹°í´ëŸ¬ìŠ¤í„° í™˜ê²½ì—ì„œ **ì¥ì•  ë³µêµ¬ë ¥**, **ë°ì´í„° ë¬´ì†ì‹¤**, **ì§€ì†ì ì¸ ì„œë¹„ìŠ¤ ì œê³µ**ì„ ë³´ì¥í•˜ëŠ” ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ¯ HA ëª©í‘œ

| í•­ëª© | ëª©í‘œ | í˜„ì¬ ë‹¬ì„± |
|-----|------|----------|
| **ì„œë¹„ìŠ¤ ê°€ìš©ì„±** | 99.9% (ì›” 43ë¶„ ë‹¤ìš´íƒ€ì„) | âœ… 99.95% |
| **ë°ì´í„° ì†ì‹¤** | 0% (RPO = 0) | âœ… 0% |
| **ë³µêµ¬ ì‹œê°„** | RTO < 5ë¶„ | âœ… ~2ë¶„ |
| **Single Point of Failure** | ì—†ìŒ | âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ HA |

---

## ğŸ—ï¸ C4 Deployment Diagram (ê³ ê°€ìš©ì„± ë°°í¬)

```mermaid
C4Deployment
    title Deployment Diagram - High Availability Architecture

    Deployment_Node(k8s_cluster01, "Kubernetes Cluster-01", "Central - 192.168.101.194") {
        Deployment_Node(ingress_pool, "Ingress Pool", "DaemonSet") {
            Container(nginx1, "Nginx Ingress-1", "Pod", "Active")
            Container(nginx2, "Nginx Ingress-2", "Pod", "Active")
        }

        Deployment_Node(receiver_pool, "Receiver StatefulSet", "HA with Replication") {
            ContainerDb(recv0, "thanos-receive-0", "Pod + PVC 30Gi", "Hashring Member<br/>RF=3")
            ContainerDb(recv1, "thanos-receive-1", "Pod + PVC 30Gi", "Hashring Member<br/>RF=3")
            ContainerDb(recv2, "thanos-receive-2", "Pod + PVC 30Gi", "Hashring Member<br/>RF=3")
        }

        Deployment_Node(query_pool, "Query Deployment", "Load Balanced") {
            Container(query0, "thanos-query-0", "Pod", "Deduplication")
            Container(query1, "thanos-query-1", "Pod", "Deduplication")
        }

        Deployment_Node(store_pool, "Store StatefulSet", "S3 Gateway") {
            ContainerDb(store0, "thanos-store-0", "Pod + Cache", "Index Cache 2Gi")
            ContainerDb(store1, "thanos-store-1", "Pod + Cache", "Index Cache 2Gi")
        }

        Deployment_Node(prometheus_pool, "Prometheus HA", "Central Monitoring") {
            ContainerDb(prom0, "prometheus-0", "Pod + PVC 50Gi", "Replica 0")
            ContainerDb(prom1, "prometheus-1", "Pod + PVC 50Gi", "Replica 1")
        }
    }

    Deployment_Node(storage, "MinIO Storage Cluster", "4 Nodes EC:4") {
        ContainerDb(minio, "MinIO Distributed", "S3 Compatible", "Erasure Coding 4:2")
    }

    Deployment_Node(k8s_cluster02, "Kubernetes Cluster-02", "Edge - 192.168.101.196") {
        Container(agent02, "prometheus-agent", "Agent Mode", "256Mi Memory")
    }

    Deployment_Node(k8s_cluster03, "Kubernetes Cluster-03", "Edge - 192.168.101.197") {
        Container(agent03, "prometheus-agent", "Agent Mode", "256Mi Memory")
    }

    Deployment_Node(k8s_cluster04, "Kubernetes Cluster-04", "Edge - 192.168.101.198") {
        Container(agent04, "prometheus-agent", "Agent Mode", "256Mi Memory")
    }

    Rel(agent02, nginx1, "Remote Write", "HTTPS Primary")
    Rel(agent02, nginx2, "Failover", "HTTPS Backup")
    Rel(agent03, nginx1, "Remote Write", "HTTPS")
    Rel(agent04, nginx2, "Remote Write", "HTTPS")

    Rel(nginx1, recv0, "Route", "Hashring")
    Rel(nginx1, recv1, "Route", "Hashring")
    Rel(nginx2, recv2, "Route", "Hashring")

    Rel(recv0, recv1, "Replicate", "gRPC RF=3")
    Rel(recv0, recv2, "Replicate", "gRPC RF=3")
    Rel(recv1, recv2, "Replicate", "gRPC RF=3")

    Rel(query0, recv0, "Query Recent", "gRPC")
    Rel(query0, store0, "Query Historical", "gRPC")
    Rel(query1, recv1, "Query Recent", "gRPC")
    Rel(query1, store1, "Query Historical", "gRPC")

    Rel(recv0, minio, "Upload Blocks", "S3 API")
    Rel(recv1, minio, "Upload Blocks", "S3 API")
    Rel(recv2, minio, "Upload Blocks", "S3 API")
    Rel(store0, minio, "Read Blocks", "S3 API")
    Rel(store1, minio, "Read Blocks", "S3 API")
    Rel(prom0, minio, "Backup", "S3 API")
    Rel(prom1, minio, "Backup", "S3 API")

    UpdateLayoutConfig($c4ShapeInRow="2", $c4BoundaryInRow="1")
```

---

## ğŸ—ï¸ HA ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "Scenario 1: Receiver-0 Failure"
        A1[Agent Remote Write] --> LB1[LoadBalancer]
        LB1 --> R0[âŒ Receiver-0 DOWN]
        LB1 --> R1[âœ… Receiver-1 Active]
        LB1 --> R2[âœ… Receiver-2 Active]

        R1 -.Still has data.-> R1
        R2 -.Still has data.-> R2

        NOTE1[Data Loss: 0%<br/>RF=3 ensures all data<br/>exists on R1, R2]
    end

    subgraph "Scenario 2: Ingress-1 Failure"
        A2[Agent Remote Write] --> LB_FAIL[âŒ Ingress-1 DOWN]
        A2 --> LB_OK[âœ… Ingress-2 Active]

        LB_OK --> R3[Receiver StatefulSet]

        NOTE2[Failover: < 10s<br/>Agent retry logic]
    end

    subgraph "Scenario 3: S3 Node Failure"
        REC[Receiver] --> S3_CLUSTER[MinIO Cluster]
        S3_CLUSTER --> S3N1[âŒ Node-1 DOWN]
        S3_CLUSTER --> S3N2[âœ… Node-2 Active]
        S3_CLUSTER --> S3N3[âœ… Node-3 Active]
        S3_CLUSTER --> S3N4[âœ… Node-4 Active]

        NOTE3[EC:4 Protection<br/>Tolerates 2 node failures]
    end

    style R0 fill:#f44336
    style R1 fill:#4caf50
    style R2 fill:#4caf50
    style LB_FAIL fill:#f44336
    style LB_OK fill:#4caf50
    style S3N1 fill:#f44336
```

---

## ğŸ—ï¸ ê¸°ì¡´ HA ì»´í¬ë„ŒíŠ¸ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "Edge Clusters (3ê°œ)"
        AGENT2[Agent cluster-02]
        AGENT3[Agent cluster-03]
        AGENT4[Agent cluster-04]
    end

    subgraph "Central Cluster - HA Components"
        subgraph "Ingress HA"
            LB1[Nginx Ingress-1]
            LB2[Nginx Ingress-2]
        end

        subgraph "Receiver HA"
            RECV0[Receiver-0<br/>TSDB]
            RECV1[Receiver-1<br/>TSDB]
            RECV2[Receiver-2<br/>TSDB]
        end

        subgraph "Prometheus HA"
            PROM0[Prometheus-0<br/>TSDB]
            PROM1[Prometheus-1<br/>TSDB]
        end

        subgraph "Query HA"
            QUERY0[Query-0]
            QUERY1[Query-1]
        end

        subgraph "Store HA"
            STORE0[Store-0]
            STORE1[Store-1]
        end

        subgraph "Storage HA"
            S3[MinIO S3<br/>4 Nodes<br/>EC:4]
        end
    end

    AGENT2 --> LB1
    AGENT2 -.Failover.-> LB2
    AGENT3 --> LB1
    AGENT4 --> LB2

    LB1 --> RECV0
    LB1 --> RECV1
    LB2 --> RECV2

    RECV0 -.Replicate.-> RECV1
    RECV0 -.Replicate.-> RECV2

    QUERY0 --> RECV0
    QUERY0 --> PROM0
    QUERY0 --> STORE0
    QUERY1 --> RECV1
    QUERY1 --> PROM1
    QUERY1 --> STORE1

    RECV0 --> S3
    PROM0 --> S3
    STORE0 --> S3

    style RECV0 fill:#4fc3f7
    style RECV1 fill:#4fc3f7
    style RECV2 fill:#4fc3f7
    style PROM0 fill:#e1bee7
    style PROM1 fill:#e1bee7
```

---

## 1ï¸âƒ£ Thanos Receiver HA (Replication)

### StatefulSet 3 Replicas

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: thanos-receive
  namespace: monitoring
spec:
  replicas: 3  # HAë¥¼ ìœ„í•œ 3ê°œ replica
  serviceName: thanos-receive
  podManagementPolicy: Parallel  # ë™ì‹œ ì‹œì‘
  template:
    spec:
      containers:
      - name: thanos-receive
        image: quay.io/thanos/thanos:v0.31.0
        args:
        - receive
        - --receive.replication-factor=3  # ëª¨ë“  replicaì— ë³µì œ
        - --receive.hashrings-file=/etc/thanos/hashrings.json
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        volumeMounts:
        - name: data
          mountPath: /data
      affinity:
        # Anti-affinity: ë‹¤ë¥¸ ë…¸ë“œì— ë°°ì¹˜
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: thanos-receive
            topologyKey: kubernetes.io/hostname
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: longhorn  # ë¶„ì‚° ìŠ¤í† ë¦¬ì§€
      resources:
        requests:
          storage: 100Gi
```

### Replication Factor = 3

```mermaid
sequenceDiagram
    participant Agent as Prometheus Agent
    participant LB as Load Balancer
    participant R0 as Receiver-0
    participant R1 as Receiver-1
    participant R2 as Receiver-2

    Agent->>LB: Remote Write Request
    LB->>R0: Primary Write

    R0->>R0: Hash(time series)
    R0->>R0: Determine primary replica

    par Replication to all replicas
        R0->>R0: Write to local TSDB
        R0->>R1: Replicate samples
        R0->>R2: Replicate samples
    end

    R1->>R1: Write to local TSDB
    R2->>R2: Write to local TSDB

    R0-->>Agent: 200 OK (after quorum)
```

### ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤

```
ì •ìƒ ìƒíƒœ:
- Receiver-0: Running âœ…
- Receiver-1: Running âœ…
- Receiver-2: Running âœ…

Receiver-0 ì¥ì• :
- Receiver-0: Down âŒ
- Receiver-1: Running âœ… (ë°ì´í„° ë³´ìœ )
- Receiver-2: Running âœ… (ë°ì´í„° ë³´ìœ )

â†’ ì¿¼ë¦¬: Receiver-1, 2ì—ì„œ ê°€ëŠ¥
â†’ ì‹ ê·œ ë°ì´í„°: Receiver-1ì´ Primaryë¡œ ìŠ¹ê²©
â†’ ë°ì´í„° ì†ì‹¤: ì—†ìŒ
â†’ RTO: 0ì´ˆ (ì¦‰ì‹œ ë³µêµ¬)
```

---

## 2ï¸âƒ£ Prometheus HA (Dual Replica)

### StatefulSet 2 Replicas

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: kube-prometheus-stack-prometheus
  namespace: monitoring
spec:
  replicas: 2  # HAë¥¼ ìœ„í•œ 2ê°œ replica

  # ì™¸ë¶€ ë ˆì´ë¸”ë¡œ replica êµ¬ë¶„
  externalLabels:
    cluster: cluster-01
    replica: $(POD_NAME)  # prometheus-0, prometheus-1

  # Storage (ê° replica ë…ë¦½ì )
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: longhorn
        resources:
          requests:
            storage: 100Gi

  # Thanos Sidecar (S3 ì—…ë¡œë“œ)
  thanos:
    image: quay.io/thanos/thanos:v0.31.0
    version: v0.31.0
    objectStorageConfig:
      key: objstore.yml
      name: thanos-objstore-secret

  # Anti-affinity
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: prometheus
        topologyKey: kubernetes.io/hostname
```

### ì¤‘ë³µ ìˆ˜ì§‘ ë° Deduplication

```mermaid
graph TB
    TARGET[Target<br/>:9100/metrics] --> PROM0[Prometheus-0<br/>replica=prometheus-0]
    TARGET --> PROM1[Prometheus-1<br/>replica=prometheus-1]

    PROM0 --> S3_0[S3 Block<br/>replica=prometheus-0]
    PROM1 --> S3_1[S3 Block<br/>replica=prometheus-1]

    S3_0 --> QUERY[Thanos Query]
    S3_1 --> QUERY

    QUERY --> DEDUP[Deduplication<br/>by replica label]
    DEDUP --> RESULT[Unique Result]
```

### Deduplication ì„¤ì •

```yaml
# Thanos Query
query:
  enabled: true
  replicas: 2
  replicaLabel:
    - replica           # Prometheus replica ë ˆì´ë¸”
    - prometheus_replica
  stores:
    - dnssrv+_grpc._tcp.prometheus-operated.monitoring.svc.cluster.local:10901
```

**ì˜ˆì‹œ**:
```promql
# Raw ë©”íŠ¸ë¦­ (2ê°œ replica)
node_cpu_seconds_total{replica="prometheus-0", cpu="0"} 12345.67
node_cpu_seconds_total{replica="prometheus-1", cpu="0"} 12345.67

# Deduplication í›„
node_cpu_seconds_total{cpu="0"} 12345.67
```

---

## 3ï¸âƒ£ Thanos Query HA

### Deployment 2 Replicas

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thanos-query
  namespace: monitoring
spec:
  replicas: 2  # HA
  selector:
    matchLabels:
      app: thanos-query
  template:
    spec:
      containers:
      - name: thanos-query
        image: quay.io/thanos/thanos:v0.31.0
        args:
        - query
        - --grpc-address=0.0.0.0:10901
        - --http-address=0.0.0.0:9090
        - --query.replica-label=replica
        - --store=dnssrv+_grpc._tcp.thanos-receive.monitoring.svc:10901
        - --store=dnssrv+_grpc._tcp.prometheus-operated.monitoring.svc:10901
        - --store=dnssrv+_grpc._tcp.thanos-store.monitoring.svc:10901
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: thanos-query
              topologyKey: kubernetes.io/hostname
---
apiVersion: v1
kind: Service
metadata:
  name: thanos-query
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 9090
    targetPort: 9090
  selector:
    app: thanos-query
```

### Load Balancing

```
Grafana â†’ Service (thanos-query:9090) â†’ Round-robin
â”œâ”€â”€ Query-0 (50% íŠ¸ë˜í”½)
â””â”€â”€ Query-1 (50% íŠ¸ë˜í”½)

Query-0 ì¥ì•  ì‹œ:
â””â”€â”€ Query-1 (100% íŠ¸ë˜í”½)
```

---

## 4ï¸âƒ£ Ingress Controller HA

### DaemonSet ë°°í¬ (ëª¨ë“  ë…¸ë“œ)

```yaml
# Nginx Ingress Controller
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
  template:
    spec:
      hostNetwork: true  # ë…¸ë“œ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
      containers:
      - name: controller
        image: registry.k8s.io/ingress-nginx/controller:v1.8.1
        args:
        - /nginx-ingress-controller
        - --election-id=ingress-controller-leader
        - --controller-class=k8s.io/ingress-nginx
        ports:
        - name: http
          containerPort: 80
          hostPort: 80
        - name: https
          containerPort: 443
          hostPort: 443
```

### Cilium L2 LoadBalancer (VIP)

```yaml
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: cluster-01-pool
spec:
  blocks:
  - start: "192.168.101.194"
    stop: "192.168.101.194"
---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: LoadBalancer
  loadBalancerIP: 192.168.101.194  # VIP
  ports:
  - name: http
    port: 80
    targetPort: 80
  - name: https
    port: 443
    targetPort: 443
  selector:
    app.kubernetes.io/name: ingress-nginx
```

**ë™ì‘**:
```
VIP: 192.168.101.194 (Cilium L2 Announcement)
â†“
Ingress Pod-1 (Node-1) 80% healthy
Ingress Pod-2 (Node-2) 20% backup
Ingress Pod-3 (Node-3) standby

Node-1 ì¥ì•  ì‹œ:
â†’ Ciliumì´ VIPë¥¼ Node-2ë¡œ ì¦‰ì‹œ ì´ë™ (< 1ì´ˆ)
```

---

## 5ï¸âƒ£ Storage HA (MinIO S3)

### MinIO Distributed Mode

```yaml
# MinIO Deployment (4 Nodes, EC:4)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
spec:
  replicas: 4
  serviceName: minio
  template:
    spec:
      containers:
      - name: minio
        image: minio/minio:latest
        args:
        - server
        - http://minio-{0...3}.minio.minio.svc.cluster.local/data
        - --console-address=:9001
        env:
        - name: MINIO_ROOT_USER
          value: minio
        - name: MINIO_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: minio-secret
              key: rootPassword
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: longhorn
      resources:
        requests:
          storage: 500Gi
```

### Erasure Coding (EC:4)

```
4ê°œ ë…¸ë“œ, EC:4 (4 data shards + 4 parity shards)

ë°ì´í„° ë¶„ì‚°:
- Node-1: Shard 1, Shard 5 (parity)
- Node-2: Shard 2, Shard 6 (parity)
- Node-3: Shard 3, Shard 7 (parity)
- Node-4: Shard 4, Shard 8 (parity)

ë‚´ê²°í•¨ì„±:
- 2ê°œ ë…¸ë“œ ë™ì‹œ ì¥ì• ê¹Œì§€ ë³µêµ¬ ê°€ëŠ¥ (N/2)
- ë°ì´í„° ì†ì‹¤ ì—†ìŒ
- Read/Write ê°€ëŠ¥

3ê°œ ë…¸ë“œ ì¥ì• :
- Read ë¶ˆê°€
- Write ë¶ˆê°€
```

---

## 6ï¸âƒ£ WAL ë° PVC ë‚´êµ¬ì„±

### Prometheus Agent WAL

```yaml
# StatefulSet with PVC
volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: longhorn  # ë¶„ì‚° ìŠ¤í† ë¦¬ì§€
      resources:
        requests:
          storage: 10Gi
```

**ì¥ì•  ë³µêµ¬**:
```
1. Agent Pod Crash
   â†’ Kubernetes ìë™ ì¬ì‹œì‘ (< 30ì´ˆ)
   â†’ PVC ìœ ì§€ë¨
   â†’ WALì—ì„œ ë³µêµ¬
   â†’ Remote Write ì¬ê°œ

2. Node Failure
   â†’ Podê°€ ë‹¤ë¥¸ ë…¸ë“œë¡œ ìŠ¤ì¼€ì¤„ë§
   â†’ PVC ì¬ë¶€ì°© (Longhorn)
   â†’ WALì—ì„œ ë³µêµ¬ (ìµœëŒ€ 4ì‹œê°„ ë°ì´í„°)
```

---

## ğŸ“Š HA ê²€ì¦ í…ŒìŠ¤íŠ¸

### 1. Receiver Pod ì‚­ì œ í…ŒìŠ¤íŠ¸

```bash
# Receiver-0 ì‚­ì œ
kubectl delete pod thanos-receive-0 -n monitoring

# ì˜ˆìƒ ê²°ê³¼
- Receiver-1, 2ì— ë°ì´í„° ì¡´ì¬ âœ…
- ì¿¼ë¦¬ ê³„ì† ê°€ëŠ¥ âœ…
- Remote Write ê³„ì† ì„±ê³µ (Receiver-1, 2) âœ…
- Receiver-0 ì¬ìƒì„± (~30ì´ˆ) âœ…
```

### 2. Prometheus Pod ì‚­ì œ í…ŒìŠ¤íŠ¸

```bash
kubectl delete pod prometheus-kube-prometheus-stack-prometheus-0 -n monitoring

# ì˜ˆìƒ ê²°ê³¼
- Prometheus-1ì—ì„œ ì¿¼ë¦¬ ê°€ëŠ¥ âœ…
- Deduplicationìœ¼ë¡œ ë°ì´í„° ì¼ê´€ì„± ìœ ì§€ âœ…
- Prometheus-0 ì¬ìƒì„± í›„ ìë™ ë³µêµ¬ âœ…
```

### 3. ì „ì²´ ë…¸ë“œ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜

```bash
# ì¤‘ì•™ í´ëŸ¬ìŠ¤í„° Master Node Drain
kubectl drain node-1 --ignore-daemonsets --delete-emptydir-data

# ì˜ˆìƒ ê²°ê³¼
- Receiver Pods â†’ Node-2, 3ìœ¼ë¡œ ì´ë™ âœ…
- Prometheus Pods â†’ Node-2, 3ìœ¼ë¡œ ì´ë™ âœ…
- Ingress VIP â†’ Node-2ë¡œ failover âœ…
- RTO: ~2ë¶„ âœ…
```

---

## ğŸš¨ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### HA ìƒíƒœ ë©”íŠ¸ë¦­

```promql
# Receiver replica ìˆ˜
count(up{job="thanos-receive"} == 1)

# Prometheus replica ìˆ˜
count(up{job="prometheus"} == 1)

# Ingress ê°€ìš©ì„±
up{job="ingress-nginx"}
```

### HA ì•Œë¦¼ ê·œì¹™

```yaml
groups:
- name: ha_alerts
  rules:
  # Receiver HA ì†ìƒ
  - alert: ThanosReceiverHADegraded
    expr: count(up{job="thanos-receive"} == 1) < 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Thanos Receiver HA degraded (< 2 replicas)"

  # Prometheus HA ì†ìƒ
  - alert: PrometheusHADegraded
    expr: count(up{job="prometheus"} == 1) < 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus HA degraded (< 2 replicas)"

  # MinIO ë…¸ë“œ ë‹¤ìš´
  - alert: MinIONodeDown
    expr: minio_cluster_nodes_online < 4
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "MinIO cluster degraded (< 4 nodes)"
```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜** â†’ [ì „ì²´-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜.md](./ì „ì²´-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜.md)
- **ë°ì´í„° íë¦„** â†’ [ë°ì´í„°-íë¦„.md](./ë°ì´í„°-íë¦„.md)
- **í™•ì¥ ì•„í‚¤í…ì²˜** â†’ [../07-í™•ì¥-ì•„í‚¤í…ì²˜/](../07-í™•ì¥-ì•„í‚¤í…ì²˜/)

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-20

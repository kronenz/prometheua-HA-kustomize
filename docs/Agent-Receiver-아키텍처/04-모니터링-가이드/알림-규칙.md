# 알림 규칙

## 📋 개요

Prometheus Alert Rules와 Alertmanager 설정을 통한 proactive 모니터링 구성입니다.

---

## 🎯 알림 우선순위

| 우선순위 | Severity | 대응 시간 | 알림 채널 |
|---------|----------|----------|----------|
| **P1** | critical | 즉시 (5분 이내) | PagerDuty, SMS, Slack |
| **P2** | warning | 30분 이내 | Slack, Email |
| **P3** | info | 업무 시간 내 | Email |

---

## 1️⃣ Remote Write 알림

### PrometheusRule CRD

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: remote-write-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack-prometheus
spec:
  groups:
  - name: remote-write
    interval: 30s
    rules:
    # Remote Write 실패
    - alert: RemoteWriteFailing
      expr: |
        rate(prometheus_remote_storage_failed_samples_total[5m])
        / rate(prometheus_remote_storage_sent_samples_total[5m])
        > 0.01
      for: 10m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Remote Write failure rate > 1% on {{ $labels.cluster }}"
        description: |
          Cluster: {{ $labels.cluster }}
          Instance: {{ $labels.instance }}
          Failure Rate: {{ $value | humanizePercentage }}

          Possible causes:
          - Thanos Receiver down
          - Network issues
          - Queue overflow
        runbook_url: "https://docs.example.com/runbooks/remote-write-failing"

    # Remote Write Queue 높음
    - alert: RemoteWriteQueueHigh
      expr: |
        prometheus_remote_storage_queue_length > 5000
      for: 15m
      labels:
        severity: warning
        component: prometheus-agent
      annotations:
        summary: "Remote Write queue length > 5000 on {{ $labels.cluster }}"
        description: |
          Queue Length: {{ $value }}
          Cluster: {{ $labels.cluster }}

          This indicates the agent cannot keep up with Remote Write.
        runbook_url: "https://docs.example.com/runbooks/queue-high"

    # Remote Write Queue Full
    - alert: RemoteWriteQueueFull
      expr: |
        prometheus_remote_storage_queue_length
        >= prometheus_remote_storage_queue_capacity * 0.95
      for: 5m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Remote Write queue almost full (>95%) on {{ $labels.cluster }}"
        description: |
          Queue: {{ $value }} / {{ prometheus_remote_storage_queue_capacity }}
          Utilization: {{ $value / prometheus_remote_storage_queue_capacity | humanizePercentage }}

    # Remote Write Shards 최대
    - alert: RemoteWriteShardsMaxed
      expr: |
        prometheus_remote_storage_shards
        >= prometheus_remote_storage_shards_max
      for: 10m
      labels:
        severity: warning
        component: prometheus-agent
      annotations:
        summary: "Remote Write shards at maximum on {{ $labels.cluster }}"
        description: |
          Shards: {{ $value }}
          Max: {{ prometheus_remote_storage_shards_max }}

          Consider increasing maxShards in configuration.

    # Remote Write 지연 증가
    - alert: RemoteWriteLatencyHigh
      expr: |
        histogram_quantile(0.99,
          rate(prometheus_remote_storage_send_duration_seconds_bucket[5m])
        ) > 2
      for: 10m
      labels:
        severity: warning
        component: prometheus-agent
      annotations:
        summary: "Remote Write latency (p99) > 2s on {{ $labels.cluster }}"
        description: "P99 Latency: {{ $value }}s"

    # Remote Write 완전 중단
    - alert: RemoteWriteStopped
      expr: |
        rate(prometheus_remote_storage_sent_samples_total[10m]) == 0
      for: 15m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Remote Write stopped on {{ $labels.cluster }}"
        description: |
          No samples sent in 15 minutes.
          Check agent logs and Receiver connectivity.
```

---

## 2️⃣ Thanos Receiver 알림

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: thanos-receiver-alerts
  namespace: monitoring
spec:
  groups:
  - name: thanos-receiver
    interval: 30s
    rules:
    # Receiver Down
    - alert: ThanosReceiverDown
      expr: |
        up{job="thanos-receive"} == 0
      for: 5m
      labels:
        severity: critical
        component: thanos-receiver
      annotations:
        summary: "Thanos Receiver {{ $labels.instance }} is down"
        description: |
          Instance: {{ $labels.instance }}
          Pod: {{ $labels.pod }}

          Check pod status:
          kubectl get pods -n monitoring -l app=thanos-receive

    # Receiver Replication 실패
    - alert: ThanosReceiverReplicationFailing
      expr: |
        rate(thanos_receive_replication_requests_total{result="error"}[5m])
        / rate(thanos_receive_replication_requests_total[5m])
        > 0.05
      for: 10m
      labels:
        severity: warning
        component: thanos-receiver
      annotations:
        summary: "Receiver replication failure rate > 5%"
        description: |
          Failure Rate: {{ $value | humanizePercentage }}
          Instance: {{ $labels.instance }}

    # Receiver Replication Factor 미달
    - alert: ThanosReceiverReplicationFactorNotMet
      expr: |
        thanos_receive_replication_factor > 0
        and
        thanos_receive_hashring_nodes{state="active"}
        < thanos_receive_replication_factor
      for: 10m
      labels:
        severity: critical
        component: thanos-receiver
      annotations:
        summary: "Receiver replication factor not met"
        description: |
          Active Nodes: {{ thanos_receive_hashring_nodes{state="active"} }}
          Required: {{ thanos_receive_replication_factor }}

          Data loss risk! Check Receiver pods.

    # Receiver Hashring 불일치
    - alert: ThanosReceiverHashringInconsistent
      expr: |
        count(thanos_receive_hashring_nodes{state="active"})
        != count(up{job="thanos-receive"} == 1)
      for: 5m
      labels:
        severity: warning
        component: thanos-receiver
      annotations:
        summary: "Receiver hashring inconsistent with actual pods"
        description: |
          Hashring Nodes: {{ count(thanos_receive_hashring_nodes{state="active"}) }}
          Running Pods: {{ count(up{job="thanos-receive"} == 1) }}

    # Receiver 높은 메모리 사용
    - alert: ThanosReceiverHighMemory
      expr: |
        container_memory_usage_bytes{pod=~"thanos-receive.*"}
        / container_spec_memory_limit_bytes{pod=~"thanos-receive.*"}
        > 0.9
      for: 10m
      labels:
        severity: warning
        component: thanos-receiver
      annotations:
        summary: "Receiver memory usage > 90% on {{ $labels.pod }}"
        description: |
          Memory: {{ $value | humanizePercentage }}
          Pod: {{ $labels.pod }}

    # Receiver S3 업로드 실패
    - alert: ThanosReceiverS3UploadFailing
      expr: |
        rate(thanos_shipper_upload_failures_total{instance=~"thanos-receive.*"}[10m])
        > 0
      for: 15m
      labels:
        severity: warning
        component: thanos-receiver
      annotations:
        summary: "Receiver S3 upload failing on {{ $labels.instance }}"
        description: |
          Check S3 connectivity and objstore.yml secret.
```

---

## 3️⃣ Agent 리소스 알림

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: agent-resource-alerts
  namespace: monitoring
spec:
  groups:
  - name: agent-resources
    interval: 1m
    rules:
    # Agent Down
    - alert: PrometheusAgentDown
      expr: |
        up{job="prometheus-agent"} == 0
      for: 5m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Prometheus Agent down on {{ $labels.cluster }}"

    # Agent 높은 메모리
    - alert: AgentHighMemory
      expr: |
        container_memory_usage_bytes{pod=~"prometheus-agent.*"}
        / container_spec_memory_limit_bytes{pod=~"prometheus-agent.*"}
        > 0.9
      for: 10m
      labels:
        severity: warning
        component: prometheus-agent
      annotations:
        summary: "Agent memory > 90% on {{ $labels.cluster }}"

    # Agent OOMKilled
    - alert: AgentOOMKilled
      expr: |
        kube_pod_container_status_restarts_total{pod=~"prometheus-agent.*"}
        and
        kube_pod_container_status_last_terminated_reason{reason="OOMKilled", pod=~"prometheus-agent.*"}
        > 0
      for: 1m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Agent OOMKilled on {{ $labels.cluster }}"
        description: |
          Pod: {{ $labels.pod }}
          Increase memory limits.

    # WAL Corruption
    - alert: AgentWALCorruption
      expr: |
        increase(prometheus_tsdb_wal_corruptions_total{job="prometheus-agent"}[1h])
        > 0
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "WAL corruption detected on {{ $labels.cluster }}"
        description: |
          Cluster: {{ $labels.cluster }}
          Check agent logs and consider pod restart.
```

---

## 4️⃣ 디스크 공간 알림

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: disk-space-alerts
  namespace: monitoring
spec:
  groups:
  - name: disk-space
    interval: 1m
    rules:
    # 디스크 공간 낮음 (Warning)
    - alert: DiskSpaceLow
      expr: |
        (1 - (node_filesystem_avail_bytes{mountpoint="/data"}
         / node_filesystem_size_bytes{mountpoint="/data"}))
        > 0.85
      for: 10m
      labels:
        severity: warning
        component: storage
      annotations:
        summary: "Disk space > 85% on {{ $labels.instance }}"
        description: |
          Usage: {{ $value | humanizePercentage }}
          Available: {{ node_filesystem_avail_bytes{mountpoint="/data"} | humanize1024 }}B

    # 디스크 공간 위험 (Critical)
    - alert: DiskSpaceCritical
      expr: |
        (1 - (node_filesystem_avail_bytes{mountpoint="/data"}
         / node_filesystem_size_bytes{mountpoint="/data"}))
        > 0.95
      for: 5m
      labels:
        severity: critical
        component: storage
      annotations:
        summary: "Disk space > 95% on {{ $labels.instance }}"
        description: |
          CRITICAL: Only {{ node_filesystem_avail_bytes{mountpoint="/data"} | humanize1024 }}B available!

          Immediate actions:
          1. Expand PVC
          2. Clean old TSDB blocks
          3. Check S3 uploads

    # 디스크 Full 예측 (24시간 이내)
    - alert: DiskWillFillIn24Hours
      expr: |
        predict_linear(node_filesystem_avail_bytes{mountpoint="/data"}[6h], 24 * 3600)
        < 0
      for: 1h
      labels:
        severity: warning
        component: storage
      annotations:
        summary: "Disk will be full in 24h on {{ $labels.instance }}"
        description: |
          Based on 6h trend, disk will be full in ~24 hours.
          Current: {{ node_filesystem_avail_bytes{mountpoint="/data"} | humanize1024 }}B
```

---

## 5️⃣ 메트릭 누락 알림

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: metric-missing-alerts
  namespace: monitoring
spec:
  groups:
  - name: metric-missing
    interval: 5m
    rules:
    # 클러스터 메트릭 누락
    - alert: ClusterMetricsMissing
      expr: |
        absent(up{cluster="cluster-03"})
        or
        count(up{cluster="cluster-03"}) == 0
      for: 15m
      labels:
        severity: critical
        component: monitoring
      annotations:
        summary: "No metrics from cluster-03"
        description: |
          Check:
          1. Prometheus Agent status
          2. Remote Write connectivity
          3. Thanos Receiver logs

    # Scrape 타겟 Down
    - alert: ScrapeTargetDown
      expr: |
        up == 0
      for: 10m
      labels:
        severity: warning
        component: scrape
      annotations:
        summary: "Scrape target {{ $labels.job }} down on {{ $labels.cluster }}"
        description: |
          Job: {{ $labels.job }}
          Instance: {{ $labels.instance }}

    # 높은 Down 타겟 비율
    - alert: HighScrapeTargetDownRate
      expr: |
        (count(up == 0) by (cluster, job)
        / count(up) by (cluster, job))
        > 0.5
      for: 10m
      labels:
        severity: critical
        component: scrape
      annotations:
        summary: "> 50% targets down for {{ $labels.job }} on {{ $labels.cluster }}"
        description: |
          Down: {{ count(up == 0) by (cluster, job) }}
          Total: {{ count(up) by (cluster, job) }}
```

---

## 6️⃣ Query 성능 알림

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: query-performance-alerts
  namespace: monitoring
spec:
  groups:
  - name: query-performance
    interval: 1m
    rules:
    # Query 느림
    - alert: ThanosQuerySlow
      expr: |
        histogram_quantile(0.99,
          rate(http_request_duration_seconds_bucket{handler="query"}[5m])
        ) > 5
      for: 10m
      labels:
        severity: warning
        component: thanos-query
      annotations:
        summary: "Thanos Query slow (p99 > 5s)"
        description: |
          P99 Duration: {{ $value }}s

          Check:
          - Store Gateway status
          - S3 connectivity
          - Query complexity

    # Query 에러 증가
    - alert: ThanosQueryErrors
      expr: |
        rate(http_requests_total{handler="query", code!~"2.."}[5m])
        > 1
      for: 10m
      labels:
        severity: warning
        component: thanos-query
      annotations:
        summary: "High query error rate"
        description: |
          Error Rate: {{ $value }} req/s
```

---

## 7️⃣ Alertmanager 설정

### Alertmanager ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

    route:
      group_by: ['alertname', 'cluster', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'default'

      routes:
      # Critical alerts → PagerDuty + Slack
      - match:
          severity: critical
        receiver: 'pagerduty-critical'
        continue: true

      - match:
          severity: critical
        receiver: 'slack-critical'

      # Warning alerts → Slack
      - match:
          severity: warning
        receiver: 'slack-warnings'

      # Info alerts → Email
      - match:
          severity: info
        receiver: 'email-notifications'

    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#monitoring-alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}'

    - name: 'pagerduty-critical'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}'

    - name: 'slack-critical'
      slack_configs:
      - channel: '#alerts-critical'
        title: ':fire: CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          *Cluster:* {{ .GroupLabels.cluster }}
          *Severity:* {{ .GroupLabels.severity }}

          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          {{ end }}
        color: 'danger'

    - name: 'slack-warnings'
      slack_configs:
      - channel: '#alerts-warnings'
        title: ':warning: Warning: {{ .GroupLabels.alertname }}'
        color: 'warning'

    - name: 'email-notifications'
      email_configs:
      - to: 'ops-team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'YOUR_PASSWORD'

    inhibit_rules:
    # 하위 알림 억제
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'cluster']
```

---

## 📊 알림 요약

| 알림 그룹 | 알림 수 | Critical | Warning | Info |
|----------|---------|----------|---------|------|
| **Remote Write** | 6 | 3 | 3 | 0 |
| **Thanos Receiver** | 6 | 3 | 3 | 0 |
| **Agent 리소스** | 4 | 3 | 1 | 0 |
| **디스크 공간** | 3 | 1 | 2 | 0 |
| **메트릭 누락** | 3 | 2 | 1 | 0 |
| **Query 성능** | 2 | 0 | 2 | 0 |
| **총합** | **24** | **12** | **12** | **0** |

---

## 🔗 관련 문서

- **핵심 메트릭** → [핵심-메트릭.md](./핵심-메트릭.md)
- **Grafana 대시보드** → [Grafana-대시보드.md](./Grafana-대시보드.md)
- **트러블슈팅** → [../03-운영-가이드/일반-트러블슈팅.md](../03-운영-가이드/일반-트러블슈팅.md)

---

**최종 업데이트**: 2025-10-20

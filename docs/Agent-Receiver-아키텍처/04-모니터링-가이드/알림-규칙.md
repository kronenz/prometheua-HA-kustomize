# ì•Œë¦¼ ê·œì¹™

## ğŸ“‹ ê°œìš”

Prometheus Alert Rulesì™€ Alertmanager ì„¤ì •ì„ í†µí•œ proactive ëª¨ë‹ˆí„°ë§ êµ¬ì„±ì…ë‹ˆë‹¤.

---

## ğŸ¯ ì•Œë¦¼ ìš°ì„ ìˆœìœ„

| ìš°ì„ ìˆœìœ„ | Severity | ëŒ€ì‘ ì‹œê°„ | ì•Œë¦¼ ì±„ë„ |
|---------|----------|----------|----------|
| **P1** | critical | ì¦‰ì‹œ (5ë¶„ ì´ë‚´) | PagerDuty, SMS, Slack |
| **P2** | warning | 30ë¶„ ì´ë‚´ | Slack, Email |
| **P3** | info | ì—…ë¬´ ì‹œê°„ ë‚´ | Email |

---

## 1ï¸âƒ£ Remote Write ì•Œë¦¼

### PrometheusRule CRD

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: remote-write-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack-prometheus
spec:
  groups:
  - name: remote-write
    interval: 30s
    rules:
    # Remote Write ì‹¤íŒ¨
    - alert: RemoteWriteFailing
      expr: |
        rate(prometheus_remote_storage_failed_samples_total[5m])
        / rate(prometheus_remote_storage_sent_samples_total[5m])
        > 0.01
      for: 10m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Remote Write failure rate > 1% on {{ $labels.cluster }}"
        description: |
          Cluster: {{ $labels.cluster }}
          Instance: {{ $labels.instance }}
          Failure Rate: {{ $value | humanizePercentage }}

          Possible causes:
          - Thanos Receiver down
          - Network issues
          - Queue overflow
        runbook_url: "https://docs.example.com/runbooks/remote-write-failing"

    # Remote Write Queue ë†’ìŒ
    - alert: RemoteWriteQueueHigh
      expr: |
        prometheus_remote_storage_queue_length > 5000
      for: 15m
      labels:
        severity: warning
        component: prometheus-agent
      annotations:
        summary: "Remote Write queue length > 5000 on {{ $labels.cluster }}"
        description: |
          Queue Length: {{ $value }}
          Cluster: {{ $labels.cluster }}

          This indicates the agent cannot keep up with Remote Write.
        runbook_url: "https://docs.example.com/runbooks/queue-high"

    # Remote Write Queue Full
    - alert: RemoteWriteQueueFull
      expr: |
        prometheus_remote_storage_queue_length
        >= prometheus_remote_storage_queue_capacity * 0.95
      for: 5m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Remote Write queue almost full (>95%) on {{ $labels.cluster }}"
        description: |
          Queue: {{ $value }} / {{ prometheus_remote_storage_queue_capacity }}
          Utilization: {{ $value / prometheus_remote_storage_queue_capacity | humanizePercentage }}

    # Remote Write Shards ìµœëŒ€
    - alert: RemoteWriteShardsMaxed
      expr: |
        prometheus_remote_storage_shards
        >= prometheus_remote_storage_shards_max
      for: 10m
      labels:
        severity: warning
        component: prometheus-agent
      annotations:
        summary: "Remote Write shards at maximum on {{ $labels.cluster }}"
        description: |
          Shards: {{ $value }}
          Max: {{ prometheus_remote_storage_shards_max }}

          Consider increasing maxShards in configuration.

    # Remote Write ì§€ì—° ì¦ê°€
    - alert: RemoteWriteLatencyHigh
      expr: |
        histogram_quantile(0.99,
          rate(prometheus_remote_storage_send_duration_seconds_bucket[5m])
        ) > 2
      for: 10m
      labels:
        severity: warning
        component: prometheus-agent
      annotations:
        summary: "Remote Write latency (p99) > 2s on {{ $labels.cluster }}"
        description: "P99 Latency: {{ $value }}s"

    # Remote Write ì™„ì „ ì¤‘ë‹¨
    - alert: RemoteWriteStopped
      expr: |
        rate(prometheus_remote_storage_sent_samples_total[10m]) == 0
      for: 15m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Remote Write stopped on {{ $labels.cluster }}"
        description: |
          No samples sent in 15 minutes.
          Check agent logs and Receiver connectivity.
```

---

## 2ï¸âƒ£ Thanos Receiver ì•Œë¦¼

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: thanos-receiver-alerts
  namespace: monitoring
spec:
  groups:
  - name: thanos-receiver
    interval: 30s
    rules:
    # Receiver Down
    - alert: ThanosReceiverDown
      expr: |
        up{job="thanos-receive"} == 0
      for: 5m
      labels:
        severity: critical
        component: thanos-receiver
      annotations:
        summary: "Thanos Receiver {{ $labels.instance }} is down"
        description: |
          Instance: {{ $labels.instance }}
          Pod: {{ $labels.pod }}

          Check pod status:
          kubectl get pods -n monitoring -l app=thanos-receive

    # Receiver Replication ì‹¤íŒ¨
    - alert: ThanosReceiverReplicationFailing
      expr: |
        rate(thanos_receive_replication_requests_total{result="error"}[5m])
        / rate(thanos_receive_replication_requests_total[5m])
        > 0.05
      for: 10m
      labels:
        severity: warning
        component: thanos-receiver
      annotations:
        summary: "Receiver replication failure rate > 5%"
        description: |
          Failure Rate: {{ $value | humanizePercentage }}
          Instance: {{ $labels.instance }}

    # Receiver Replication Factor ë¯¸ë‹¬
    - alert: ThanosReceiverReplicationFactorNotMet
      expr: |
        thanos_receive_replication_factor > 0
        and
        thanos_receive_hashring_nodes{state="active"}
        < thanos_receive_replication_factor
      for: 10m
      labels:
        severity: critical
        component: thanos-receiver
      annotations:
        summary: "Receiver replication factor not met"
        description: |
          Active Nodes: {{ thanos_receive_hashring_nodes{state="active"} }}
          Required: {{ thanos_receive_replication_factor }}

          Data loss risk! Check Receiver pods.

    # Receiver Hashring ë¶ˆì¼ì¹˜
    - alert: ThanosReceiverHashringInconsistent
      expr: |
        count(thanos_receive_hashring_nodes{state="active"})
        != count(up{job="thanos-receive"} == 1)
      for: 5m
      labels:
        severity: warning
        component: thanos-receiver
      annotations:
        summary: "Receiver hashring inconsistent with actual pods"
        description: |
          Hashring Nodes: {{ count(thanos_receive_hashring_nodes{state="active"}) }}
          Running Pods: {{ count(up{job="thanos-receive"} == 1) }}

    # Receiver ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
    - alert: ThanosReceiverHighMemory
      expr: |
        container_memory_usage_bytes{pod=~"thanos-receive.*"}
        / container_spec_memory_limit_bytes{pod=~"thanos-receive.*"}
        > 0.9
      for: 10m
      labels:
        severity: warning
        component: thanos-receiver
      annotations:
        summary: "Receiver memory usage > 90% on {{ $labels.pod }}"
        description: |
          Memory: {{ $value | humanizePercentage }}
          Pod: {{ $labels.pod }}

    # Receiver S3 ì—…ë¡œë“œ ì‹¤íŒ¨
    - alert: ThanosReceiverS3UploadFailing
      expr: |
        rate(thanos_shipper_upload_failures_total{instance=~"thanos-receive.*"}[10m])
        > 0
      for: 15m
      labels:
        severity: warning
        component: thanos-receiver
      annotations:
        summary: "Receiver S3 upload failing on {{ $labels.instance }}"
        description: |
          Check S3 connectivity and objstore.yml secret.
```

---

## 3ï¸âƒ£ Agent ë¦¬ì†ŒìŠ¤ ì•Œë¦¼

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: agent-resource-alerts
  namespace: monitoring
spec:
  groups:
  - name: agent-resources
    interval: 1m
    rules:
    # Agent Down
    - alert: PrometheusAgentDown
      expr: |
        up{job="prometheus-agent"} == 0
      for: 5m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Prometheus Agent down on {{ $labels.cluster }}"

    # Agent ë†’ì€ ë©”ëª¨ë¦¬
    - alert: AgentHighMemory
      expr: |
        container_memory_usage_bytes{pod=~"prometheus-agent.*"}
        / container_spec_memory_limit_bytes{pod=~"prometheus-agent.*"}
        > 0.9
      for: 10m
      labels:
        severity: warning
        component: prometheus-agent
      annotations:
        summary: "Agent memory > 90% on {{ $labels.cluster }}"

    # Agent OOMKilled
    - alert: AgentOOMKilled
      expr: |
        kube_pod_container_status_restarts_total{pod=~"prometheus-agent.*"}
        and
        kube_pod_container_status_last_terminated_reason{reason="OOMKilled", pod=~"prometheus-agent.*"}
        > 0
      for: 1m
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "Agent OOMKilled on {{ $labels.cluster }}"
        description: |
          Pod: {{ $labels.pod }}
          Increase memory limits.

    # WAL Corruption
    - alert: AgentWALCorruption
      expr: |
        increase(prometheus_tsdb_wal_corruptions_total{job="prometheus-agent"}[1h])
        > 0
      labels:
        severity: critical
        component: prometheus-agent
      annotations:
        summary: "WAL corruption detected on {{ $labels.cluster }}"
        description: |
          Cluster: {{ $labels.cluster }}
          Check agent logs and consider pod restart.
```

---

## 4ï¸âƒ£ ë””ìŠ¤í¬ ê³µê°„ ì•Œë¦¼

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: disk-space-alerts
  namespace: monitoring
spec:
  groups:
  - name: disk-space
    interval: 1m
    rules:
    # ë””ìŠ¤í¬ ê³µê°„ ë‚®ìŒ (Warning)
    - alert: DiskSpaceLow
      expr: |
        (1 - (node_filesystem_avail_bytes{mountpoint="/data"}
         / node_filesystem_size_bytes{mountpoint="/data"}))
        > 0.85
      for: 10m
      labels:
        severity: warning
        component: storage
      annotations:
        summary: "Disk space > 85% on {{ $labels.instance }}"
        description: |
          Usage: {{ $value | humanizePercentage }}
          Available: {{ node_filesystem_avail_bytes{mountpoint="/data"} | humanize1024 }}B

    # ë””ìŠ¤í¬ ê³µê°„ ìœ„í—˜ (Critical)
    - alert: DiskSpaceCritical
      expr: |
        (1 - (node_filesystem_avail_bytes{mountpoint="/data"}
         / node_filesystem_size_bytes{mountpoint="/data"}))
        > 0.95
      for: 5m
      labels:
        severity: critical
        component: storage
      annotations:
        summary: "Disk space > 95% on {{ $labels.instance }}"
        description: |
          CRITICAL: Only {{ node_filesystem_avail_bytes{mountpoint="/data"} | humanize1024 }}B available!

          Immediate actions:
          1. Expand PVC
          2. Clean old TSDB blocks
          3. Check S3 uploads

    # ë””ìŠ¤í¬ Full ì˜ˆì¸¡ (24ì‹œê°„ ì´ë‚´)
    - alert: DiskWillFillIn24Hours
      expr: |
        predict_linear(node_filesystem_avail_bytes{mountpoint="/data"}[6h], 24 * 3600)
        < 0
      for: 1h
      labels:
        severity: warning
        component: storage
      annotations:
        summary: "Disk will be full in 24h on {{ $labels.instance }}"
        description: |
          Based on 6h trend, disk will be full in ~24 hours.
          Current: {{ node_filesystem_avail_bytes{mountpoint="/data"} | humanize1024 }}B
```

---

## 5ï¸âƒ£ ë©”íŠ¸ë¦­ ëˆ„ë½ ì•Œë¦¼

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: metric-missing-alerts
  namespace: monitoring
spec:
  groups:
  - name: metric-missing
    interval: 5m
    rules:
    # í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ëˆ„ë½
    - alert: ClusterMetricsMissing
      expr: |
        absent(up{cluster="cluster-03"})
        or
        count(up{cluster="cluster-03"}) == 0
      for: 15m
      labels:
        severity: critical
        component: monitoring
      annotations:
        summary: "No metrics from cluster-03"
        description: |
          Check:
          1. Prometheus Agent status
          2. Remote Write connectivity
          3. Thanos Receiver logs

    # Scrape íƒ€ê²Ÿ Down
    - alert: ScrapeTargetDown
      expr: |
        up == 0
      for: 10m
      labels:
        severity: warning
        component: scrape
      annotations:
        summary: "Scrape target {{ $labels.job }} down on {{ $labels.cluster }}"
        description: |
          Job: {{ $labels.job }}
          Instance: {{ $labels.instance }}

    # ë†’ì€ Down íƒ€ê²Ÿ ë¹„ìœ¨
    - alert: HighScrapeTargetDownRate
      expr: |
        (count(up == 0) by (cluster, job)
        / count(up) by (cluster, job))
        > 0.5
      for: 10m
      labels:
        severity: critical
        component: scrape
      annotations:
        summary: "> 50% targets down for {{ $labels.job }} on {{ $labels.cluster }}"
        description: |
          Down: {{ count(up == 0) by (cluster, job) }}
          Total: {{ count(up) by (cluster, job) }}
```

---

## 6ï¸âƒ£ Query ì„±ëŠ¥ ì•Œë¦¼

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: query-performance-alerts
  namespace: monitoring
spec:
  groups:
  - name: query-performance
    interval: 1m
    rules:
    # Query ëŠë¦¼
    - alert: ThanosQuerySlow
      expr: |
        histogram_quantile(0.99,
          rate(http_request_duration_seconds_bucket{handler="query"}[5m])
        ) > 5
      for: 10m
      labels:
        severity: warning
        component: thanos-query
      annotations:
        summary: "Thanos Query slow (p99 > 5s)"
        description: |
          P99 Duration: {{ $value }}s

          Check:
          - Store Gateway status
          - S3 connectivity
          - Query complexity

    # Query ì—ëŸ¬ ì¦ê°€
    - alert: ThanosQueryErrors
      expr: |
        rate(http_requests_total{handler="query", code!~"2.."}[5m])
        > 1
      for: 10m
      labels:
        severity: warning
        component: thanos-query
      annotations:
        summary: "High query error rate"
        description: |
          Error Rate: {{ $value }} req/s
```

---

## 7ï¸âƒ£ Alertmanager ì„¤ì •

### Alertmanager ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

    route:
      group_by: ['alertname', 'cluster', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'default'

      routes:
      # Critical alerts â†’ PagerDuty + Slack
      - match:
          severity: critical
        receiver: 'pagerduty-critical'
        continue: true

      - match:
          severity: critical
        receiver: 'slack-critical'

      # Warning alerts â†’ Slack
      - match:
          severity: warning
        receiver: 'slack-warnings'

      # Info alerts â†’ Email
      - match:
          severity: info
        receiver: 'email-notifications'

    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#monitoring-alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}'

    - name: 'pagerduty-critical'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}'

    - name: 'slack-critical'
      slack_configs:
      - channel: '#alerts-critical'
        title: ':fire: CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          *Cluster:* {{ .GroupLabels.cluster }}
          *Severity:* {{ .GroupLabels.severity }}

          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          {{ end }}
        color: 'danger'

    - name: 'slack-warnings'
      slack_configs:
      - channel: '#alerts-warnings'
        title: ':warning: Warning: {{ .GroupLabels.alertname }}'
        color: 'warning'

    - name: 'email-notifications'
      email_configs:
      - to: 'ops-team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'YOUR_PASSWORD'

    inhibit_rules:
    # í•˜ìœ„ ì•Œë¦¼ ì–µì œ
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'cluster']
```

---

## ğŸ“Š ì•Œë¦¼ ìš”ì•½

| ì•Œë¦¼ ê·¸ë£¹ | ì•Œë¦¼ ìˆ˜ | Critical | Warning | Info |
|----------|---------|----------|---------|------|
| **Remote Write** | 6 | 3 | 3 | 0 |
| **Thanos Receiver** | 6 | 3 | 3 | 0 |
| **Agent ë¦¬ì†ŒìŠ¤** | 4 | 3 | 1 | 0 |
| **ë””ìŠ¤í¬ ê³µê°„** | 3 | 1 | 2 | 0 |
| **ë©”íŠ¸ë¦­ ëˆ„ë½** | 3 | 2 | 1 | 0 |
| **Query ì„±ëŠ¥** | 2 | 0 | 2 | 0 |
| **ì´í•©** | **24** | **12** | **12** | **0** |

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **í•µì‹¬ ë©”íŠ¸ë¦­** â†’ [í•µì‹¬-ë©”íŠ¸ë¦­.md](./í•µì‹¬-ë©”íŠ¸ë¦­.md)
- **Grafana ëŒ€ì‹œë³´ë“œ** â†’ [Grafana-ëŒ€ì‹œë³´ë“œ.md](./Grafana-ëŒ€ì‹œë³´ë“œ.md)
- **íŠ¸ëŸ¬ë¸”ìŠˆíŒ…** â†’ [../03-ìš´ì˜-ê°€ì´ë“œ/ì¼ë°˜-íŠ¸ëŸ¬ë¸”ìŠˆíŒ….md](../03-ìš´ì˜-ê°€ì´ë“œ/ì¼ë°˜-íŠ¸ëŸ¬ë¸”ìŠˆíŒ….md)

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-20

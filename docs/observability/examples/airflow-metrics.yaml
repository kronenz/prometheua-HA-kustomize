# Apache Airflow Metrics 수집 예제
# StatsD Exporter를 통한 Prometheus 메트릭 수집

---
# StatsD Exporter Mapping ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-statsd-mapping
  namespace: data-team
data:
  mapping.yaml: |
    mappings:
      # DAG 처리 메트릭
      - match: "airflow.dag_processing.last_duration.*"
        name: "airflow_dag_processing_duration_seconds"
        labels:
          dag_id: "$1"

      - match: "airflow.dag_processing.last_run.seconds_ago.*"
        name: "airflow_dag_processing_last_run_seconds"
        labels:
          dag_id: "$1"

      # Task Instance 메트릭
      - match: "airflow.ti_successes"
        name: "airflow_task_instance_successes_total"

      - match: "airflow.ti_failures"
        name: "airflow_task_instance_failures_total"

      - match: "airflow.ti.start.*.*"
        name: "airflow_task_instance_start_total"
        labels:
          dag_id: "$1"
          task_id: "$2"

      - match: "airflow.ti.finish.*.*.*"
        name: "airflow_task_instance_finish_total"
        labels:
          dag_id: "$1"
          task_id: "$2"
          state: "$3"

      # DAG Run 메트릭
      - match: "airflow.dagrun.duration.success.*"
        name: "airflow_dagrun_duration_seconds"
        labels:
          dag_id: "$1"
          state: "success"

      - match: "airflow.dagrun.duration.failed.*"
        name: "airflow_dagrun_duration_seconds"
        labels:
          dag_id: "$1"
          state: "failed"

      # Scheduler 메트릭
      - match: "airflow.scheduler.heartbeat"
        name: "airflow_scheduler_heartbeat"

      - match: "airflow.scheduler.tasks.running"
        name: "airflow_scheduler_tasks_running"

      - match: "airflow.scheduler.tasks.starving"
        name: "airflow_scheduler_tasks_starving"

      - match: "airflow.scheduler.tasks.executable"
        name: "airflow_scheduler_tasks_executable"

      # Executor 메트릭
      - match: "airflow.executor.open_slots"
        name: "airflow_executor_open_slots"

      - match: "airflow.executor.queued_tasks"
        name: "airflow_executor_queued_tasks"

      - match: "airflow.executor.running_tasks"
        name: "airflow_executor_running_tasks"

      # Pool 메트릭
      - match: "airflow.pool.open_slots.*"
        name: "airflow_pool_open_slots"
        labels:
          pool: "$1"

      - match: "airflow.pool.used_slots.*"
        name: "airflow_pool_used_slots"
        labels:
          pool: "$1"

---
# Airflow Webserver Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
  namespace: data-team
  labels:
    app: airflow
    component: webserver
    service-team: data-team
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
      component: webserver
  template:
    metadata:
      labels:
        app: airflow
        component: webserver
        service-team: data-team
    spec:
      containers:
        # Airflow Webserver
        - name: airflow
          image: apache/airflow:2.8.0
          ports:
            - name: http
              containerPort: 8080
          env:
            # StatsD 메트릭 활성화
            - name: AIRFLOW__METRICS__STATSD_ON
              value: "True"
            - name: AIRFLOW__METRICS__STATSD_HOST
              value: "localhost"
            - name: AIRFLOW__METRICS__STATSD_PORT
              value: "8125"
            - name: AIRFLOW__METRICS__STATSD_PREFIX
              value: "airflow"
          resources:
            requests:
              memory: "2Gi"
              cpu: "1"
            limits:
              memory: "4Gi"
              cpu: "2"

        # StatsD Exporter (Sidecar)
        - name: statsd-exporter
          image: prom/statsd-exporter:v0.26.0
          ports:
            - name: metrics
              containerPort: 9102
            - name: statsd
              containerPort: 8125
              protocol: UDP
          args:
            - --statsd.listen-udp=:8125
            - --web.listen-address=:9102
            - --statsd.mapping-config=/etc/statsd-exporter/mapping.yaml
          volumeMounts:
            - name: statsd-mapping
              mountPath: /etc/statsd-exporter
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "200m"

      volumes:
        - name: statsd-mapping
          configMap:
            name: airflow-statsd-mapping

---
# Airflow Scheduler Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  namespace: data-team
  labels:
    app: airflow
    component: scheduler
    service-team: data-team
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
      component: scheduler
  template:
    metadata:
      labels:
        app: airflow
        component: scheduler
        service-team: data-team
    spec:
      containers:
        # Airflow Scheduler
        - name: airflow
          image: apache/airflow:2.8.0
          command:
            - airflow
            - scheduler
          env:
            # StatsD 메트릭 활성화
            - name: AIRFLOW__METRICS__STATSD_ON
              value: "True"
            - name: AIRFLOW__METRICS__STATSD_HOST
              value: "localhost"
            - name: AIRFLOW__METRICS__STATSD_PORT
              value: "8125"
            - name: AIRFLOW__METRICS__STATSD_PREFIX
              value: "airflow"
          resources:
            requests:
              memory: "2Gi"
              cpu: "1"
            limits:
              memory: "4Gi"
              cpu: "2"

        # StatsD Exporter (Sidecar)
        - name: statsd-exporter
          image: prom/statsd-exporter:v0.26.0
          ports:
            - name: metrics
              containerPort: 9102
            - name: statsd
              containerPort: 8125
              protocol: UDP
          args:
            - --statsd.listen-udp=:8125
            - --web.listen-address=:9102
            - --statsd.mapping-config=/etc/statsd-exporter/mapping.yaml
          volumeMounts:
            - name: statsd-mapping
              mountPath: /etc/statsd-exporter
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "200m"

      volumes:
        - name: statsd-mapping
          configMap:
            name: airflow-statsd-mapping

---
# Airflow Webserver Service
apiVersion: v1
kind: Service
metadata:
  name: airflow-webserver
  namespace: data-team
  labels:
    app: airflow
    component: webserver
    service-team: data-team
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: webserver
  ports:
    - name: http
      port: 8080
      targetPort: 8080
    - name: metrics
      port: 9102
      targetPort: 9102

---
# Airflow Scheduler Service
apiVersion: v1
kind: Service
metadata:
  name: airflow-scheduler
  namespace: data-team
  labels:
    app: airflow
    component: scheduler
    service-team: data-team
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: scheduler
  ports:
    - name: metrics
      port: 9102
      targetPort: 9102

---
# Airflow Webserver ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: airflow-webserver-metrics
  namespace: data-team
  labels:
    release: kube-prometheus-stack
    app: airflow
    service-team: data-team
spec:
  selector:
    matchLabels:
      app: airflow
      component: webserver
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scrapeTimeout: 10s

---
# Airflow Scheduler ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: airflow-scheduler-metrics
  namespace: data-team
  labels:
    release: kube-prometheus-stack
    app: airflow
    service-team: data-team
spec:
  selector:
    matchLabels:
      app: airflow
      component: scheduler
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scrapeTimeout: 10s

---
# 주요 Airflow 메트릭 설명
#
# DAG 처리 메트릭:
# - airflow_dag_processing_duration_seconds: DAG 파싱 시간
# - airflow_dag_processing_last_run_seconds: 마지막 DAG 파싱 이후 시간
#
# Task Instance 메트릭:
# - airflow_task_instance_successes_total: 성공한 Task 수 (누적)
# - airflow_task_instance_failures_total: 실패한 Task 수 (누적)
# - airflow_task_instance_start_total: 시작된 Task 수 (DAG/Task별)
# - airflow_task_instance_finish_total: 완료된 Task 수 (DAG/Task/State별)
#
# DAG Run 메트릭:
# - airflow_dagrun_duration_seconds: DAG 실행 시간 (성공/실패별)
#
# Scheduler 메트릭:
# - airflow_scheduler_heartbeat: Scheduler Heartbeat (장애 감지용)
# - airflow_scheduler_tasks_running: 실행 중인 Task 수
# - airflow_scheduler_tasks_starving: 리소스 부족으로 대기 중인 Task 수
# - airflow_scheduler_tasks_executable: 실행 가능한 Task 수
#
# Executor 메트릭:
# - airflow_executor_open_slots: 사용 가능한 슬롯 수
# - airflow_executor_queued_tasks: 대기 중인 Task 수
# - airflow_executor_running_tasks: 실행 중인 Task 수
#
# Pool 메트릭:
# - airflow_pool_open_slots: Pool의 사용 가능한 슬롯 수
# - airflow_pool_used_slots: Pool의 사용 중인 슬롯 수
#
# PromQL 쿼리 예제:
# - Task 실패율: rate(airflow_task_instance_failures_total[5m]) / (rate(airflow_task_instance_successes_total[5m]) + rate(airflow_task_instance_failures_total[5m]))
# - DAG 처리 시간 (p95): histogram_quantile(0.95, rate(airflow_dag_processing_duration_seconds[5m]))
# - Executor 사용률: (airflow_executor_running_tasks / (airflow_executor_open_slots + airflow_executor_running_tasks)) * 100
# - Scheduler 장애 감지: delta(airflow_scheduler_heartbeat[2m]) == 0

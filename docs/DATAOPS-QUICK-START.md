# DataOps ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸ“‹ ë¬¸ì„œ ê°œìš”

ë¹…ë°ì´í„° DataOps í”Œë«í¼ì˜ End-to-End ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

### ì™„ì„±ëœ ë¬¸ì„œ

| ë¬¸ì„œ | í¬ê¸° | ìš©ë„ |
|------|------|------|
| [README-DATAOPS-MONITORING.md](./README-DATAOPS-MONITORING.md) | 446 lines | ì „ì²´ ê°œìš” ë° ë¹ ë¥¸ ì‹œì‘ |
| [dataops-monitoring-architecture.md](./dataops-monitoring-architecture.md) | 503 lines | ìƒì„¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ |
| [dataops-expert-meeting-notes.md](./dataops-expert-meeting-notes.md) | 656 lines | SRE/ì—”ì§€ë‹ˆì–´ ì „ë¬¸ê°€ íšŒì˜ë¡ |
| [dataops-implementation-guide.md](./dataops-implementation-guide.md) | 960 lines | êµ¬í˜„ ê°€ì´ë“œ (ì½”ë“œ í¬í•¨) |

**ì´ ë¬¸ì„œëŸ‰**: 2,565 lines (ì•½ 75KB)

---

## ğŸ¯ ì‹œìŠ¤í…œ ì„¤ê³„ ìš”ì•½

### ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ ì‹œìŠ¤í…œ

```
ì‚¬ìš©ì Portal
    â†“
GitOps (Bitbucket â†’ Jenkins â†’ ArgoCD)
    â†“
Kubernetes Cluster
    â†“
Application (Spark, Airflow, Trino)
    â†“
Data Lake (Apache Iceberg + Hive Metastore)
    â†“
Storage (S3/MinIO, Oracle DB, Isilon NAS, Ceph)
```

### 6ë‹¨ê³„ ëª¨ë‹ˆí„°ë§ êµ¬ì¡°

| ë‹¨ê³„ | ëª©í‘œ ì§ˆë¬¸ | ì£¼ìš” ë©”íŠ¸ë¦­ |
|------|----------|------------|
| **1. GitOps ë°°í¬** | ë°°í¬ê°€ ì™„ë£Œë˜ì—ˆëŠ”ê°€? | Jenkins ë¹Œë“œ, ArgoCD Sync, Pod Readiness |
| **2. ë°°í¬ ê²€ì¦** | ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì‘ë™í•˜ëŠ”ê°€? | Pod Status, Liveness/Readiness Probe |
| **3. ë¦¬ì†ŒìŠ¤ ê°€ìš©ëŸ‰** | ì‹¤í–‰ ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ê°€ ìˆëŠ”ê°€? | CPU/Memory/Storage ê°€ìš©ëŸ‰ |
| **4. ì›Œí¬ë¡œë“œ ì‹¤í–‰** | Jobì´ ì •ìƒ ì‹¤í–‰ë˜ëŠ”ê°€? | Spark/Airflow/Trino ì„±ê³µë¥ , Duration |
| **5. ë°ì´í„° íŒŒì´í”„ë¼ì¸** | ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì €ì¥/ì½ê¸°ë˜ëŠ”ê°€? | Iceberg, S3 Latency, Metastore ì‘ë‹µ ì‹œê°„ |
| **6. E2E í†µí•©** | ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ SLOë¥¼ ì¤€ìˆ˜í•˜ëŠ”ê°€? | Pipeline Completion Time, Success Rate |

---

## ğŸ—ï¸ ëŒ€ì‹œë³´ë“œ ê³„ì¸µ êµ¬ì¡°

```mermaid
graph TB
    subgraph "Level 0: Main Navigation"
        Nav[ë©”ì¸ ë„¤ë¹„ê²Œì´ì…˜ í—ˆë¸Œ<br/>4ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ + 6ê°œ ë„ë©”ì¸]
    end

    subgraph "Level 1: Domain Dashboards"
        D1[1. GitOps ë°°í¬ íŒŒì´í”„ë¼ì¸]
        D2[2. ë¦¬ì†ŒìŠ¤ ê°€ìš©ëŸ‰]
        D3[3. ì›Œí¬ë¡œë“œ ì‹¤í–‰]
        D4[4. ë°ì´í„° íŒŒì´í”„ë¼ì¸]
        D5[5. ìµœì í™” & íŠ¸ëŸ¬ë¸”ìŠˆíŒ…]
        D6[6. E2E Analytics]
    end

    subgraph "Level 2: Detailed Drill-down"
        DD1[ìƒì„¸ ë©”íŠ¸ë¦­]
        DD2[ë¡œê·¸ ìƒê´€ê´€ê³„]
        DD3[ì—ëŸ¬ ë¶„ì„]
    end

    Nav --> D1 & D2 & D3 & D4 & D5 & D6
    D1 & D2 & D3 & D4 & D5 & D6 --> DD1 & DD2 & DD3
```

### ëŒ€ì‹œë³´ë“œ ëª©ë¡

| UID | ì œëª© | ìƒíƒœ |
|-----|------|------|
| `dataops-main-nav` | ğŸŒ DataOps Platform - Main Navigation | âœ… ìƒì„±ë¨ |
| `dataops-gitops-pipeline` | ğŸ”„ GitOps ë°°í¬ íŒŒì´í”„ë¼ì¸ | ğŸ“ ì„¤ê³„ ì™„ë£Œ |
| `dataops-resource-capacity` | ğŸ’¾ ë¦¬ì†ŒìŠ¤ ê°€ìš©ëŸ‰ | ğŸ“ ì„¤ê³„ ì™„ë£Œ |
| `dataops-workload-execution` | âš™ï¸ ì›Œí¬ë¡œë“œ ì‹¤í–‰ | ğŸ“ ì„¤ê³„ ì™„ë£Œ |
| `dataops-data-pipeline` | ğŸ—„ï¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ | ğŸ“ ì„¤ê³„ ì™„ë£Œ |
| `dataops-optimization` | ğŸ”§ ìµœì í™” & íŠ¸ëŸ¬ë¸”ìŠˆíŒ… | ğŸ“ ì„¤ê³„ ì™„ë£Œ |
| `dataops-e2e-analytics` | ğŸ“Š E2E Analytics | ğŸ“ ì„¤ê³„ ì™„ë£Œ |

---

## ğŸ“ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì „ëµ

### Exporter êµ¬ì„±

| Component | Exporter | Scrape Interval | Namespace |
|-----------|----------|-----------------|-----------|
| **Jenkins** | prometheus-plugin | 1m | ci-cd |
| **ArgoCD** | built-in | 30s | argocd |
| **Spark** | JMX Exporter | 15s | spark |
| **Airflow** | StatsD Exporter | 30s | airflow |
| **Trino** | built-in | 30s | trino |
| **Iceberg** | Custom Python | 5m | data-lake |
| **Hive Metastore** | JMX Exporter | 1m | data-lake |
| **S3/MinIO** | built-in | 1m | storage |
| **Oracle DB** | oracledb_exporter | 1m | database |
| **Ceph** | ceph-exporter | 1m | storage |
| **Isilon** | Custom REST API | 5m | storage |

---

## ğŸ¯ SLO ë° Alert ì •ì±…

### Service Level Objectives

```
Availability SLO: 99.9%
â†’ Error Budget: 0.1% = 43.2ë¶„/ì›”
â†’ MTTD ëª©í‘œ: < 5ë¶„ (í‰ê·  ì¥ì•  ê°ì§€ ì‹œê°„)
â†’ MTTR ëª©í‘œ: < 30ë¶„ (í‰ê·  ì¥ì•  ë³µêµ¬ ì‹œê°„)
```

### Alert ìš°ì„ ìˆœìœ„

| ë“±ê¸‰ | ëŒ€ì‘ ì‹œê°„ | ì±„ë„ | ì˜ˆì‹œ |
|------|----------|------|------|
| **P1 (Critical)** | ì¦‰ì‹œ | PagerDuty + Slack | í”Œë«í¼ ë‹¤ìš´, ë°ì´í„° ì†ì‹¤ |
| **P2 (High)** | 30ë¶„ | Slack + Email | ë¦¬ì†ŒìŠ¤ ë¶€ì¡±, Job ì‹¤íŒ¨ìœ¨ ê¸‰ì¦ |
| **P3 (Medium)** | 2ì‹œê°„ | Slack | ìŠ¬ë¡œìš° ì¿¼ë¦¬, Scheduler ì§€ì—° |
| **P4 (Low)** | 24ì‹œê°„ | Email | ìš©ëŸ‰ ì˜ˆì¸¡ ê²½ê³  |

### Multi-window Burn Rate Alerting

```yaml
# Fast Burn (1ì‹œê°„ ë‚´ 5% Error Budget ì†Œì§„ ì˜ˆìƒ)
- alert: DataOpsErrorBudgetFastBurn
  expr: |
    (1 - (
      sum(rate(dataops_pipeline_success_total[1h]))
      /
      sum(rate(dataops_pipeline_total[1h]))
    )) > (0.1 * 5)  # 99.9% SLOì˜ 5ë°°
  severity: critical

# Slow Burn (6ì‹œê°„ ë‚´ 10% Error Budget ì†Œì§„ ì˜ˆìƒ)
- alert: DataOpsErrorBudgetSlowBurn
  expr: |
    (1 - (
      sum(rate(dataops_pipeline_success_total[6h]))
      /
      sum(rate(dataops_pipeline_total[6h]))
    )) > (0.1 * 1.67)  # 99.9% SLOì˜ 1.67ë°°
  severity: warning
```

---

## ğŸš€ 8ì£¼ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: Foundation (Week 1-2)

**ëª©í‘œ**: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¸í”„ë¼ êµ¬ì¶•

- [ ] Prometheus/Thanos ì„¤ì • ê²€ì¦
- [ ] ServiceMonitor ìƒì„± (Jenkins, ArgoCD, Spark, Airflow, Trino)
- [ ] JMX Exporter ë°°í¬ (Spark, Hive Metastore)
- [ ] Custom Exporter ê°œë°œ (Iceberg, Isilon)
- [ ] Recording Rules ì •ì˜

**Deliverables**:
- 11ê°œ ServiceMonitor YAML
- JMX Exporter ConfigMap
- Python Iceberg Exporter (Docker ì´ë¯¸ì§€)
- 20ê°œ Recording Rules

---

### Phase 2: Core Dashboards (Week 3-4)

**ëª©í‘œ**: í•µì‹¬ ëŒ€ì‹œë³´ë“œ ê°œë°œ

- [ ] Main Navigation ëŒ€ì‹œë³´ë“œ ë°°í¬ (âœ… ì´ë¯¸ ìƒì„±ë¨)
- [ ] GitOps Pipeline ëŒ€ì‹œë³´ë“œ
  - Bitbucket webhook ìƒíƒœ
  - Jenkins ë¹Œë“œ ì„±ê³µë¥ /ì‹¤íŒ¨ ì¶”ì´
  - ArgoCD Sync Status
  - Pod Readiness íƒ€ì„ë¼ì¸
- [ ] Resource Capacity ëŒ€ì‹œë³´ë“œ
  - CPU/Memory ê°€ìš©ëŸ‰ (í´ëŸ¬ìŠ¤í„°/ë…¸ë“œ/ë„¤ì„ìŠ¤í˜ì´ìŠ¤)
  - Storage ì‚¬ìš©ë¥  (Longhorn, Ceph, Isilon)
  - Network ëŒ€ì—­í­
- [ ] Workload Execution ëŒ€ì‹œë³´ë“œ
  - Spark Job ì„±ê³µë¥ , Duration, GC Time
  - Airflow DAG Run ìƒíƒœ, Task Duration
  - Trino Query ì„±ê³µë¥ , Wall Time

**Deliverables**:
- 4ê°œ Grafana Dashboard JSON
- ConfigMap YAML íŒŒì¼
- Sidecar ìë™ ë¡œë”© ì„¤ì •

---

### Phase 3: Advanced Features (Week 5-6)

**ëª©í‘œ**: ê³ ê¸‰ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

- [ ] Data Pipeline ëŒ€ì‹œë³´ë“œ
  - Iceberg Table Metadata Size, Snapshot Count
  - S3 GET/PUT Latency, Error Rate
  - Hive Metastore Response Time
  - Oracle Connection Pool, Query Duration
- [ ] Optimization ëŒ€ì‹œë³´ë“œ
  - Small Files ë¶„ì„ (Iceberg)
  - GC Time Ratio (Spark)
  - Slow Query ëª©ë¡ (Trino, Oracle)
  - Cost Analysis (CPU-hour, Storage GB-month)
- [ ] E2E Analytics ëŒ€ì‹œë³´ë“œ
  - Pipeline Completion Time (P50/P95/P99)
  - Success Rate Trend
  - SLO Dashboard (Error Budget Burn Rate)
- [ ] Alert Rules ì„¤ì •
  - 40ê°œ PrometheusRule ìƒì„±
  - AlertManager ë¼ìš°íŒ… ì„¤ì •
  - Slack/Email í†µí•©

**Deliverables**:
- 3ê°œ Grafana Dashboard JSON
- 40ê°œ Alert Rules (PrometheusRule)
- AlertManager Config

---

### Phase 4: Optimization & Go-Live (Week 7-8)

**ëª©í‘œ**: ì„±ëŠ¥ ìµœì í™” ë° ìš´ì˜ ì¤€ë¹„

- [ ] Recording Rules ìµœì í™”
  - ëŠë¦° ì¿¼ë¦¬ ì‹ë³„ ë° Pre-computation
  - Cardinality ë¶„ì„ ë° Label ìµœì í™”
- [ ] ìë™ ë¦¬í¬íŠ¸ ìƒì„±
  - ì¼ê°„/ì£¼ê°„/ì›”ê°„ ë¦¬í¬íŠ¸ (Grafana Reporter)
  - SLO ë‹¬ì„±ë¥  ë¦¬í¬íŠ¸
- [ ] Auto-remediation ì„¤ì •
  - Runbook Automation (Ansible Tower ì—°ë™)
  - PagerDuty í†µí•©
- [ ] ìš´ì˜ ë¬¸ì„œí™”
  - Runbook ì‘ì„± (20ê°œ ì‹œë‚˜ë¦¬ì˜¤)
  - On-call Playbook
- [ ] ì‚¬ìš©ì êµìœ¡
  - ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²• (Data Engineer)
  - Alert ëŒ€ì‘ ì ˆì°¨ (SRE)
- [ ] Go-Live

**Deliverables**:
- ìµœì í™”ëœ Recording Rules
- Grafana Reporter í…œí”Œë¦¿
- 20ê°œ Runbook ë¬¸ì„œ
- On-call Playbook
- êµìœ¡ ìë£Œ

---

## ğŸ’¾ ë°ì´í„° ë³´ê´€ ì •ì±…

### Prometheus/Thanos Retention

| ë°ì´í„° íƒ€ì… | ë³´ê´€ ê¸°ê°„ | ìŠ¤í† ë¦¬ì§€ | ìš©ë„ |
|------------|----------|---------|------|
| **Raw Metrics** | 15ì¼ | Prometheus Local | ì‹¤ì‹œê°„ ì¿¼ë¦¬ |
| **5ë¶„ Downsampled** | 90ì¼ | S3 (Thanos Store) | ìµœê·¼ ì¶”ì„¸ ë¶„ì„ |
| **1ì‹œê°„ Downsampled** | 1ë…„ | S3 (Thanos Store) | ì¥ê¸° ì¶”ì„¸, ìš©ëŸ‰ ê³„íš |

### Recording Rules ì „ëµ

```yaml
# 1ë¶„ ì§‘ê³„ (1ì¼ ë³´ê´€)
- record: job:spark_job_duration_seconds:p95_1m
  expr: histogram_quantile(0.95, rate(spark_job_duration_seconds_bucket[1m]))

# 5ë¶„ ì§‘ê³„ (7ì¼ ë³´ê´€)
- record: job:spark_job_duration_seconds:p95_5m
  expr: histogram_quantile(0.95, rate(spark_job_duration_seconds_bucket[5m]))

# 1ì‹œê°„ ì§‘ê³„ (90ì¼ ë³´ê´€)
- record: job:spark_job_duration_seconds:p95_1h
  expr: histogram_quantile(0.95, rate(spark_job_duration_seconds_bucket[1h]))
```

---

## ğŸ” í•µì‹¬ ë©”íŠ¸ë¦­ ì˜ˆì œ

### Spark Job Monitoring

```promql
# Job ì„±ê³µë¥  (ìµœê·¼ 24ì‹œê°„)
sum(rate(spark_job_status{status="SUCCEEDED"}[24h]))
/
sum(rate(spark_job_status[24h]))

# Job Duration P95 (ìµœê·¼ 1ì‹œê°„)
histogram_quantile(0.95, 
  sum by (le) (rate(spark_job_duration_seconds_bucket[1h]))
)

# GC Time Ratio (ëª©í‘œ: < 10%)
sum(rate(jvm_gc_collection_seconds_sum[5m]))
/
sum(rate(jvm_gc_collection_seconds_count[5m]))
```

### Airflow DAG Monitoring

```promql
# DAG Run ì„±ê³µë¥ 
sum(rate(airflow_dag_run_status{status="success"}[1h]))
/
sum(rate(airflow_dag_run_status[1h]))

# Task Duration P95
histogram_quantile(0.95,
  sum by (le, dag_id) (rate(airflow_task_duration_seconds_bucket[1h]))
)

# Scheduler Lag (ëª©í‘œ: < 30s)
airflow_scheduler_heartbeat_seconds - time()
```

### Trino Query Monitoring

```promql
# Query ì„±ê³µë¥ 
sum(rate(trino_query_completed{state="FINISHED"}[5m]))
/
sum(rate(trino_query_completed[5m]))

# Query Wall Time P95 (ëª©í‘œ: < 10ë¶„)
histogram_quantile(0.95,
  sum by (le) (rate(trino_query_wall_time_seconds_bucket[5m]))
)

# Worker ê°€ìš©ì„±
count(up{job="trino-worker"} == 1)
/
count(up{job="trino-worker"})
```

### Iceberg Table Monitoring

```promql
# Small Files Ratio (ëª©í‘œ: < 30%)
iceberg_table_small_files_count
/
iceberg_table_total_files_count

# Snapshot Count (ëª©í‘œ: < 100)
iceberg_table_snapshot_count

# Table Size Growth Rate (GB/day)
rate(iceberg_table_size_bytes[24h]) / 1024 / 1024 / 1024
```

### S3/MinIO Monitoring

```promql
# GET Latency P95 (ëª©í‘œ: < 100ms)
histogram_quantile(0.95,
  sum by (le) (rate(s3_request_duration_seconds_bucket{operation="GET"}[5m]))
)

# Error Rate (ëª©í‘œ: < 0.1%)
sum(rate(s3_request_errors_total[5m]))
/
sum(rate(s3_requests_total[5m]))

# Bandwidth Utilization
rate(s3_bytes_sent_total[1m]) + rate(s3_bytes_received_total[1m])
```

---

## ğŸ› ï¸ êµ¬í˜„ ì‹œì‘í•˜ê¸°

### 1. ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸

```bash
# Kubernetes í´ëŸ¬ìŠ¤í„° ì ‘ê·¼
kubectl cluster-info

# Prometheus Operator ì„¤ì¹˜ í™•ì¸
kubectl get crd prometheuses.monitoring.coreos.com

# Thanos Query ì ‘ê·¼ í™•ì¸
kubectl port-forward -n monitoring svc/thanos-query 9090:9090
# http://localhost:9090
```

### 2. ServiceMonitor ë°°í¬

```bash
# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p monitoring/servicemonitors

# Spark ServiceMonitor ìƒì„±
kubectl apply -f - <<EOF
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: spark-jobs
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: spark
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics

# DataOps í”Œë«í¼ End-to-End ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

## ğŸŒ ê°œìš”

ì´ ë¬¸ì„œëŠ” ë¹…ë°ì´í„° DataOps í”Œë«í¼ì˜ ì „ì²´ ìƒëª…ì£¼ê¸°ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì¢…í•© ì‹œìŠ¤í…œì— ëŒ€í•œ ì„¤ê³„ ë° êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.

### ì‹œìŠ¤í…œ ë²”ìœ„

```
ì‚¬ìš©ì Portal â†’ GitOps (Bitbucket/Jenkins/ArgoCD) â†’
Application (Spark/Airflow/Trino) â†’
Data Lake (Iceberg) â†’
Storage (S3/Oracle/Isilon/Ceph)
```

### ì£¼ìš” ëª©í‘œ

- âœ… **6ê°œ ê³„ì¸µ ëª¨ë‹ˆí„°ë§**: ë°°í¬ë¶€í„° ìŠ¤í† ë¦¬ì§€ê¹Œì§€ ì „ êµ¬ê°„
- âœ… **End-to-End ì¶”ì **: ì‚¬ìš©ì ìš”ì²­ë¶€í„° ë°ì´í„° ì €ì¥ê¹Œì§€
- âœ… **99.9% ê°€ìš©ì„±**: ì›” 43.2ë¶„ ì´í•˜ ë‹¤ìš´íƒ€ì„
- âœ… **MTTD < 5ë¶„**: ì¥ì•  ë°œìƒ 5ë¶„ ë‚´ ê°ì§€
- âœ… **MTTR < 30ë¶„**: ì¥ì•  ë°œìƒ 30ë¶„ ë‚´ ë³µêµ¬

---

## ğŸ“š ë¬¸ì„œ êµ¬ì¡°

| ë¬¸ì„œ | ì„¤ëª… | ëŒ€ìƒ |
|------|------|------|
| **[dataops-monitoring-architecture.md](./dataops-monitoring-architecture.md)** | ì „ì²´ ì•„í‚¤í…ì²˜ ë° ìš”êµ¬ì‚¬í•­ ë¶„ì„ | ì „ì²´ íŒ€ |
| **[dataops-expert-meeting-notes.md](./dataops-expert-meeting-notes.md)** | SRE/ì—”ì§€ë‹ˆì–´ ì „ë¬¸ê°€ íšŒì˜ë¡ | ì˜ì‚¬ê²°ì •ì |
| **[dataops-implementation-guide.md](./dataops-implementation-guide.md)** | ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ | êµ¬í˜„ ë‹´ë‹¹ì |

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ìš”ì•½

### ê³„ì¸µë³„ ëª¨ë‹ˆí„°ë§

```mermaid
graph TB
    subgraph "Level 0: Main Navigation"
        Nav[ë©”ì¸ ë„¤ë¹„ê²Œì´ì…˜ í—ˆë¸Œ<br/>4ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ + 6ê°œ ë„ë©”ì¸]
    end

    subgraph "Level 1: Domain Dashboards"
        D1[1. GitOps ë°°í¬ íŒŒì´í”„ë¼ì¸]
        D2[2. ë¦¬ì†ŒìŠ¤ ê°€ìš©ëŸ‰]
        D3[3. ì›Œí¬ë¡œë“œ ì‹¤í–‰]
        D4[4. ë°ì´í„° íŒŒì´í”„ë¼ì¸]
        D5[5. ìµœì í™” & íŠ¸ëŸ¬ë¸”ìŠˆíŒ…]
        D6[6. E2E Analytics]
    end

    subgraph "Level 2: Detailed Drill-down"
        DD1[ìƒì„¸ ë©”íŠ¸ë¦­]
        DD2[ë¡œê·¸ ìƒê´€ê´€ê³„]
        DD3[ì—ëŸ¬ ë¶„ì„]
    end

    Nav --> D1 & D2 & D3 & D4 & D5 & D6
    D1 & D2 & D3 & D4 & D5 & D6 --> DD1 & DD2 & DD3
```

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì „ëµ

| Layer | Component | Exporter | Interval |
|-------|-----------|----------|----------|
| **GitOps** | Jenkins | prometheus-plugin | 1m |
| | ArgoCD | built-in | 30s |
| **Application** | Spark | JMX Exporter | 15s |
| | Airflow | StatsD Exporter | 30s |
| | Trino | built-in | 30s |
| **Data** | Iceberg | Custom Exporter | 5m |
| | Hive Metastore | JMX Exporter | 1m |
| **Storage** | S3/MinIO | built-in | 1m |
| | Oracle | oracledb_exporter | 1m |
| | Ceph | ceph-exporter | 1m |
| | Isilon | Custom REST API | 5m |

---

## ğŸ¯ 6ë‹¨ê³„ ëª¨ë‹ˆí„°ë§ ìƒì„¸

### 1ë‹¨ê³„: GitOps ë°°í¬ íŒŒì´í”„ë¼ì¸

**ëª©í‘œ**: ë°°í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ê°€?

**ì£¼ìš” ë©”íŠ¸ë¦­**:
- Jenkins ë¹Œë“œ ì„±ê³µë¥  (ëª©í‘œ: > 95%)
- ArgoCD Sync ìƒíƒœ (ëª©í‘œ: Healthy)
- Pod Readiness (ëª©í‘œ: 100%)
- ë°°í¬ ì†Œìš” ì‹œê°„ (ëª©í‘œ: < 10ë¶„)

**ì•Œë¦¼ ì¡°ê±´**:
- ğŸ”´ ë¹Œë“œ ì‹¤íŒ¨ 3íšŒ ì—°ì†
- ğŸ”´ ArgoCD Out of Sync > 5ë¶„
- ğŸ”´ Pod CrashLoopBackOff

---

### 2ë‹¨ê³„: ë°°í¬ ê²€ì¦

**ëª©í‘œ**: ë°°í¬ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ê°€?

**ì£¼ìš” ë©”íŠ¸ë¦­**:
- Pod Running/Pending/Failed ìƒíƒœ
- Liveness/Readiness Probe
- Container ì¬ì‹œì‘ íšŸìˆ˜
- Pod ì‹œì‘ ì†Œìš” ì‹œê°„

**ì•Œë¦¼ ì¡°ê±´**:
- ğŸ”´ Pod Phase != Running
- ğŸ”´ Restart > 3íšŒ
- ğŸŸ¡ Init Time > 5ë¶„

---

### 3ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ê°€ìš©ëŸ‰

**ëª©í‘œ**: ì›Œí¬ë¡œë“œ ì‹¤í–‰ì— ì¶©ë¶„í•œ ë¦¬ì†ŒìŠ¤ê°€ ìˆëŠ”ê°€?

**ì£¼ìš” ë©”íŠ¸ë¦­**:
- CPU ê°€ìš©ëŸ‰ (ëª©í‘œ: > 20% ì—¬ìœ )
- ë©”ëª¨ë¦¬ ê°€ìš©ëŸ‰ (ëª©í‘œ: > 15% ì—¬ìœ )
- ìŠ¤í† ë¦¬ì§€ ìš©ëŸ‰ (ëª©í‘œ: < 80% ì‚¬ìš©)
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ (ëª©í‘œ: < 70% ì‚¬ìš©)

**ì•Œë¦¼ ì¡°ê±´**:
- ğŸ”´ CPU > 85%
- ğŸ”´ Memory > 90%
- ğŸ”´ Storage > 85%
- ğŸ”´ OOM Kill ë°œìƒ

---

### 4ë‹¨ê³„: ì›Œí¬ë¡œë“œ ì‹¤í–‰

**ëª©í‘œ**: Jobì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ê³  ì™„ë£Œë˜ëŠ”ê°€?

#### Spark
- Active Jobs
- Failed Jobs (ëª©í‘œ: < 2% in 24h)
- Stage Duration (P95 < SLA)
- GC Time Ratio (ëª©í‘œ: < 10%)

#### Airflow
- DAG Run Success Rate (ëª©í‘œ: > 95%)
- Task Duration (P95 < SLA)
- Scheduler Lag (ëª©í‘œ: < 30s)

#### Trino
- Query Success Rate (ëª©í‘œ: > 99%)
- Query Wall Time (P95 < 10m)
- Worker Availability

**ì•Œë¦¼ ì¡°ê±´**:
- ğŸ”´ Job ì‹¤íŒ¨ìœ¨ > 5%
- ğŸŸ¡ ì‹¤í–‰ ì‹œê°„ > SLA + 50%
- ğŸŸ¡ GC Time > 20%

---

### 5ë‹¨ê³„: ë°ì´í„° íŒŒì´í”„ë¼ì¸

**ëª©í‘œ**: ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì €ì¥/ì½ê¸°ë˜ëŠ”ê°€?

**ì£¼ìš” ë©”íŠ¸ë¦­**:

#### Iceberg
- Table Metadata Size
- Snapshot Count (ëª©í‘œ: < 100)
- Small Files Ratio (ëª©í‘œ: < 30%)

#### S3/MinIO
- GET/PUT Latency (ëª©í‘œ: < 100ms)
- Error Rate (ëª©í‘œ: < 0.1%)

#### Hive Metastore
- Response Time (ëª©í‘œ: < 1s)
- Connection Pool (ëª©í‘œ: < 80%)

#### Oracle DB
- Connection Pool (ëª©í‘œ: < 90%)
- Query Duration (ëª©í‘œ: < 5s)
- Tablespace Usage (ëª©í‘œ: < 85%)

**ì•Œë¦¼ ì¡°ê±´**:
- ğŸ”´ S3 Error Rate > 1%
- ğŸŸ¡ Metastore Response > 3s
- ğŸ”´ Oracle Tablespace > 90%

---

### 6ë‹¨ê³„: End-to-End í†µí•©

**ëª©í‘œ**: ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ SLOë¥¼ ì¤€ìˆ˜í•˜ëŠ”ê°€?

**ì£¼ìš” ë©”íŠ¸ë¦­**:
- Pipeline Completion Time (Portal â†’ Data)
- Data Processing Latency (P50/P95/P99)
- Overall Success Rate
- Data Volume Processed

**SLI/SLO**:
- Availability: 99.9% (43ë¶„ downtime/month)
- Latency: P95 < 1 hour
- Success Rate: > 98%
- Error Budget: 0.1%

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸

```bash
# Kubernetes í´ëŸ¬ìŠ¤í„° ì ‘ê·¼
kubectl cluster-info

# Prometheus Operator ì„¤ì¹˜ í™•ì¸
kubectl get crd prometheuses.monitoring.coreos.com

# Grafana ì„¤ì¹˜ í™•ì¸
kubectl get deployment -n monitoring kube-prometheus-stack-grafana
```

### 2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¸í”„ë¼ ë°°í¬

```bash
# ServiceMonitor ë°°í¬
kubectl apply -f monitoring/servicemonitors/

# Recording Rules ë°°í¬
kubectl apply -f monitoring/recording-rules/

# Alert Rules ë°°í¬
kubectl apply -f monitoring/alert-rules/
```

### 3. ëŒ€ì‹œë³´ë“œ ë°°í¬

```bash
# ConfigMap ìƒì„±
kubectl apply -k deploy-new/overlays/cluster-01-central/kube-prometheus-stack/

# Grafana ì¬ì‹œì‘
kubectl rollout restart deployment -n monitoring kube-prometheus-stack-grafana

# ì ‘ì†
# http://grafana.k8s-cluster-01.miribit.lab
# Username: admin / Password: admin123
```

### 4. ê²€ì¦

```bash
# Prometheus Targets í™•ì¸
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
# http://localhost:9090/targets

# ëŒ€ì‹œë³´ë“œ í™•ì¸
kubectl logs -n monitoring -l app.kubernetes.io/name=grafana -c grafana-sc-dashboard

# Alert í™•ì¸
kubectl port-forward -n monitoring svc/kube-prometheus-stack-alertmanager 9093:9093
# http://localhost:9093
```

---

## ğŸ“Š ëŒ€ì‹œë³´ë“œ ëª©ë¡

| ëŒ€ì‹œë³´ë“œ | UID | ì„¤ëª… |
|---------|-----|------|
| **Main Navigation** | `dataops-main-nav` | ì „ì²´ í”Œë«í¼ ê°œìš” ë° ë„¤ë¹„ê²Œì´ì…˜ |
| **GitOps Pipeline** | `dataops-gitops-pipeline` | Bitbucket â†’ Jenkins â†’ ArgoCD |
| **Resource Capacity** | `dataops-resource-capacity` | CPU/Memory/Storage/Network |
| **Workload Execution** | `dataops-workload-execution` | Spark/Airflow/Trino |
| **Data Pipeline** | `dataops-data-pipeline` | Iceberg/S3/Hive/Oracle |
| **Optimization** | `dataops-optimization` | ì„±ëŠ¥ ë¶„ì„, ì—ëŸ¬ ì¶”ì , ë¹„ìš© |
| **E2E Analytics** | `dataops-e2e-analytics` | SLI/SLO, ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ |

---

## ğŸš¨ ì•Œë¦¼ ì •ì±…

### ìš°ì„ ìˆœìœ„

| ë“±ê¸‰ | ëŒ€ì‘ ì‹œê°„ | ì±„ë„ | ì˜ˆì‹œ |
|------|----------|------|------|
| **P1 (Critical)** | ì¦‰ì‹œ | PagerDuty + Slack | í”Œë«í¼ ë‹¤ìš´, ë°ì´í„° ì†ì‹¤ |
| **P2 (High)** | 30ë¶„ | Slack + Email | ë¦¬ì†ŒìŠ¤ ë¶€ì¡±, Job ì‹¤íŒ¨ìœ¨ ê¸‰ì¦ |
| **P3 (Medium)** | 2ì‹œê°„ | Slack | ìŠ¬ë¡œìš° ì¿¼ë¦¬, Scheduler ì§€ì—° |
| **P4 (Low)** | 24ì‹œê°„ | Email | ìš©ëŸ‰ ì˜ˆì¸¡ ê²½ê³  |

### SLO ë° Error Budget

```
SLO: 99.9% ê°€ìš©ì„±
â†’ Error Budget: 0.1% = 43.2ë¶„/ì›”

Burn Rate ì•Œë¦¼:
- Fast Burn (1h): 5% Error Budget ì†Œì§„ ì˜ˆìƒ â†’ P1
- Slow Burn (6h): 10% Error Budget ì†Œì§„ ì˜ˆìƒ â†’ P2
```

---

## ğŸ“ˆ ì˜ˆìƒ íš¨ê³¼

### ê¸°ìˆ  ì§€í‘œ

| ì§€í‘œ | í˜„ì¬ | ëª©í‘œ | ê°œì„  |
|------|------|------|------|
| **MTTD** | 30ë¶„ | 5ë¶„ | -83% |
| **MTTR** | 2ì‹œê°„ | 30ë¶„ | -75% |
| **ì¥ì•  ë¹ˆë„** | 10íšŒ/ì›” | 5íšŒ/ì›” | -50% |
| **ì•Œë¦¼ ì •í™•ë„** | 60% | 95% | +58% |

### ë¹„ì¦ˆë‹ˆìŠ¤ íš¨ê³¼

- **ìš´ì˜ íš¨ìœ¨ì„±**: +30% (On-call ì‹œê°„ ê°ì†Œ)
- **ë¹„ìš© ì ˆê°**: 15% (ë¦¬ì†ŒìŠ¤ ìµœì í™”)
- **ì‚¬ìš©ì ë§Œì¡±ë„**: 4.0/5.0 ëª©í‘œ
- **í”Œë«í¼ í™œìš©ë„**: +25%

---

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

1. **ë©”íŠ¸ë¦­ì´ ìˆ˜ì§‘ë˜ì§€ ì•Šì„ ë•Œ**
   ```bash
   # Prometheus Targets í™•ì¸
   kubectl get servicemonitor -n monitoring
   kubectl get prometheus -n monitoring
   ```

2. **ëŒ€ì‹œë³´ë“œê°€ ë¡œë“œë˜ì§€ ì•Šì„ ë•Œ**
   ```bash
   # ConfigMap í™•ì¸
   kubectl get cm -n monitoring -l grafana_dashboard=1

   # Sidecar ë¡œê·¸
   kubectl logs -n monitoring -l app.kubernetes.io/name=grafana -c grafana-sc-dashboard
   ```

3. **ì•Œë¦¼ì´ ë°œì†¡ë˜ì§€ ì•Šì„ ë•Œ**
   ```bash
   # AlertManager ìƒíƒœ
   kubectl get alertmanager -n monitoring

   # Alert Rule í™•ì¸
   kubectl get prometheusrule -n monitoring
   ```

ìì„¸í•œ ë‚´ìš©ì€ **[dataops-implementation-guide.md](./dataops-implementation-guide.md#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)** ì°¸ì¡°

---

## ğŸ“… êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: Foundation (Week 1-2)
- [ ] Prometheus/Thanos ì„¤ì •
- [ ] ServiceMonitor ìƒì„±
- [ ] JMX Exporter ë°°í¬
- [ ] Recording Rules ì •ì˜

### Phase 2: Core Dashboards (Week 3-4)
- [ ] Main Navigation ê°œë°œ
- [ ] GitOps Pipeline ëŒ€ì‹œë³´ë“œ
- [ ] Resource Capacity ëŒ€ì‹œë³´ë“œ
- [ ] Workload Execution ëŒ€ì‹œë³´ë“œ

### Phase 3: Advanced Features (Week 5-6)
- [ ] Data Pipeline ëŒ€ì‹œë³´ë“œ
- [ ] Optimization ëŒ€ì‹œë³´ë“œ
- [ ] E2E Analytics ëŒ€ì‹œë³´ë“œ
- [ ] Alert Rules ì„¤ì •
- [ ] SLO Dashboard

### Phase 4: Optimization (Week 7-8)
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] ìë™ ë¦¬í¬íŠ¸
- [ ] Auto-remediation
- [ ] Go-Live

---

## ğŸ‘¥ ë‹´ë‹¹ì

| ì—­í•  | ë‹´ë‹¹ì | ì±…ì„ |
|------|--------|------|
| **í”„ë¡œì íŠ¸ ë¦¬ë”** | SRE Lead | ì „ì²´ í”„ë¡œì íŠ¸ ê´€ë¦¬ |
| **ì•„í‚¤í…íŠ¸** | Platform Engineer | ì•„í‚¤í…ì²˜ ì„¤ê³„ |
| **êµ¬í˜„** | DevOps Team | ì¸í”„ë¼ êµ¬ì¶• |
| **ëŒ€ì‹œë³´ë“œ** | Data Visualization Team | Grafana ëŒ€ì‹œë³´ë“œ ê°œë°œ |
| **í…ŒìŠ¤íŠ¸** | QA Team | ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ |

---

## ğŸ“– ì°¸ê³  ìë£Œ

### ë‚´ë¶€ ë¬¸ì„œ
- [ì „ì²´ ì•„í‚¤í…ì²˜](./dataops-monitoring-architecture.md)
- [ì „ë¬¸ê°€ íšŒì˜ë¡](./dataops-expert-meeting-notes.md)
- [êµ¬í˜„ ê°€ì´ë“œ](./dataops-implementation-guide.md)

### ì™¸ë¶€ ì°¸ê³ 
- [Google SRE Workbook](https://sre.google/workbook/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/)
- [Grafana Dashboard Guide](https://grafana.com/docs/grafana/latest/best-practices/)
- [Thanos Architecture](https://thanos.io/tip/thanos/design.md/)

---

## ğŸ‰ ì‹œì‘í•˜ê¸°

```bash
# 1. ë¬¸ì„œ ì½ê¸°
cat docs/dataops-monitoring-architecture.md
cat docs/dataops-expert-meeting-notes.md
cat docs/dataops-implementation-guide.md

# 2. Phase 1 ì‹œì‘
kubectl apply -f monitoring/servicemonitors/
kubectl apply -f monitoring/recording-rules/

# 3. ëŒ€ì‹œë³´ë“œ ë°°í¬
kubectl apply -k deploy-new/overlays/cluster-01-central/kube-prometheus-stack/

# 4. Grafana ì ‘ì†
echo "http://grafana.k8s-cluster-01.miribit.lab"
echo "Username: admin / Password: admin123"
```

**Good Luck! ğŸš€**

---

**ë¬¸ì„œ ë²„ì „**: v1.0
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-05
**í”„ë¡œì íŠ¸**: DataOps Platform Monitoring
**Team**: Platform Engineering & SRE

# Spark ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê°€ì´ë“œ

## ğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ í˜„í™©

**ë°°í¬ ì™„ë£Œ ì¼ì‹œ**: 2025-11-07
**Prometheus ìˆ˜ì§‘ ìƒíƒœ**: âœ… ì •ìƒ

---

## 1. Spark Master ë©”íŠ¸ë¦­

### í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë©”íŠ¸ë¦­

```promql
# ì‚´ì•„ìˆëŠ” Worker ìˆ˜
metrics_master_aliveWorkers_Value{namespace="spark"}

# í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜
metrics_master_apps_Value{namespace="spark"}

# ëŒ€ê¸° ì¤‘ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜
metrics_master_waitingApps_Value{namespace="spark"}

# ì „ì²´ Worker ìˆ˜
metrics_master_workers_Value{namespace="spark"}
```

**í˜„ì¬ ê°’**:
- `metrics_master_aliveWorkers_Value = 2` (2ê°œ Worker ì‹¤í–‰ ì¤‘)
- `metrics_master_apps_Value = 0` (í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì•± ì—†ìŒ)
- `metrics_master_waitingApps_Value = 0` (ëŒ€ê¸° ì¤‘ì¸ ì•± ì—†ìŒ)
- `metrics_master_workers_Value = 4` (ì „ì²´ Worker ë ˆì§€ìŠ¤íŠ¸ë¦¬)

### Hive Catalog ë©”íŠ¸ë¦­

```promql
# íŒŒì¼ ìºì‹œ íˆíŠ¸
metrics_HiveExternalCatalog_fileCacheHits_Count{namespace="spark"}

# ë°œê²¬ëœ íŒŒì¼ ìˆ˜
metrics_HiveExternalCatalog_filesDiscovered_Count{namespace="spark"}

# Hive í´ë¼ì´ì–¸íŠ¸ í˜¸ì¶œ ìˆ˜
metrics_HiveExternalCatalog_hiveClientCalls_Count{namespace="spark"}
```

### ì½”ë“œ ìƒì„± ë©”íŠ¸ë¦­

```promql
# ì»´íŒŒì¼ ì‹œê°„ (íˆìŠ¤í† ê·¸ë¨)
metrics_CodeGenerator_compilationTime_Count{namespace="spark"}
metrics_CodeGenerator_compilationTime_Mean{namespace="spark"}
metrics_CodeGenerator_compilationTime_95thPercentile{namespace="spark"}

# ìƒì„±ëœ í´ë˜ìŠ¤ í¬ê¸°
metrics_CodeGenerator_generatedClassSize_Count{namespace="spark"}
metrics_CodeGenerator_generatedClassSize_Mean{namespace="spark"}
```

---

## 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì‹œ ì¶”ê°€ ë©”íŠ¸ë¦­

Spark Job ì‹¤í–‰ ì¤‘ì—ëŠ” ë‹¤ìŒ ë©”íŠ¸ë¦­ë“¤ì´ ì¶”ê°€ë¡œ ìˆ˜ì§‘ë©ë‹ˆë‹¤:

### Executor ë©”íŠ¸ë¦­

```promql
# Executor ìˆ˜
metrics_master_apps_running_Value{namespace="spark"}

# Task ì‹¤í–‰ ë©”íŠ¸ë¦­
metrics_executor_runTime_*
metrics_executor_shuffleWrite_*
metrics_executor_shuffleRead_*
```

### Driver ë©”íŠ¸ë¦­

```promql
# Driver ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
metrics_driver_*_memoryUsed_*
metrics_driver_*_diskUsed_*
```

### Stage & Task ë©”íŠ¸ë¦­

```promql
# Stage ì™„ë£Œìœ¨
metrics_stage_*_completedTasks_*
metrics_stage_*_failedTasks_*

# Task ì‹¤í–‰ ì‹œê°„
metrics_task_*_executorRunTime_*
metrics_task_*_executorCpuTime_*
```

---

## 3. ëŒ€ì‹œë³´ë“œ PromQL ì˜ˆì œ

### Spark í´ëŸ¬ìŠ¤í„° ìƒíƒœ íŒ¨ë„

**ì œëª©**: Spark Cluster Status
**PromQL**:
```promql
# Worker ê°€ìš©ì„±
metrics_master_aliveWorkers_Value{namespace="spark"}

# ì‹¤í–‰ ì¤‘ì¸ ì•±
metrics_master_apps_Value{namespace="spark"}
```

**ì‹œê°í™”**: Stat íŒ¨ë„
**ì„¤ëª…**: Spark í´ëŸ¬ìŠ¤í„°ì˜ ì „ì²´ ìƒíƒœ (ì‚´ì•„ìˆëŠ” Workerì™€ ì‹¤í–‰ ì¤‘ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜)

### Worker ì¶”ì„¸ ê·¸ë˜í”„

**ì œëª©**: Spark Workers Over Time
**PromQL**:
```promql
metrics_master_aliveWorkers_Value{namespace="spark"}
```

**ì‹œê°í™”**: Time series
**ì„¤ëª…**: ì‹œê°„ëŒ€ë³„ Spark Worker ê°€ìš©ì„± ë³€í™”

### ì• í”Œë¦¬ì¼€ì´ì…˜ ì²˜ë¦¬ëŸ‰

**ì œëª©**: Spark Application Throughput
**PromQL**:
```promql
# ì™„ë£Œëœ ì•± ë¹„ìœ¨
rate(metrics_master_apps_Value{namespace="spark"}[5m])

# ëŒ€ê¸° ì•± ì¶”ì„¸
metrics_master_waitingApps_Value{namespace="spark"}
```

**ì‹œê°í™”**: Graph
**ì„¤ëª…**: Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ì²˜ë¦¬ ì†ë„ ë° ëŒ€ê¸°ì—´ ìƒíƒœ

### ì½”ë“œ ìƒì„± ì„±ëŠ¥

**ì œëª©**: Code Generation Performance
**PromQL**:
```promql
# í‰ê·  ì»´íŒŒì¼ ì‹œê°„
metrics_CodeGenerator_compilationTime_Mean{namespace="spark"}

# 95th ë°±ë¶„ìœ„ ì»´íŒŒì¼ ì‹œê°„
metrics_CodeGenerator_compilationTime_95thPercentile{namespace="spark"}
```

**ì‹œê°í™”**: Gauge
**ì„¤ëª…**: Spark ë™ì  ì½”ë“œ ìƒì„±ì˜ ì„±ëŠ¥ ì§€í‘œ

---

## 4. Kubernetes ê¸°ë°˜ ë©”íŠ¸ë¦­

Spark Podì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì€ Kubernetes ë©”íŠ¸ë¦­ìœ¼ë¡œ ìˆ˜ì§‘ë©ë‹ˆë‹¤:

### Pod ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

```promql
# CPU ì‚¬ìš©ë¥ 
rate(container_cpu_usage_seconds_total{namespace="spark"}[5m])

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
container_memory_working_set_bytes{namespace="spark"}

# ë„¤íŠ¸ì›Œí¬ I/O
rate(container_network_receive_bytes_total{namespace="spark"}[5m])
rate(container_network_transmit_bytes_total{namespace="spark"}[5m])
```

### Pod ìƒíƒœ

```promql
# Pod Ready ìƒíƒœ
kube_pod_status_ready{namespace="spark"}

# Pod Restart ìˆ˜
kube_pod_container_status_restarts_total{namespace="spark"}

# Pod ë‹¨ê³„ (Running, Pending, Failed)
kube_pod_status_phase{namespace="spark"}
```

### Persistent Volume

```promql
# PVC ë°”ì¸ë”© ìƒíƒœ
kube_persistentvolumeclaim_status_phase{namespace="spark"}

# ë³¼ë¥¨ ì‚¬ìš©ëŸ‰ (Longhorn)
kubelet_volume_stats_used_bytes{namespace="spark"}
kubelet_volume_stats_capacity_bytes{namespace="spark"}
```

---

## 5. í…ŒìŠ¤íŠ¸ ì‘ì—… ì‹¤í–‰

### Spark Pi ì˜ˆì œ ì‹¤í–‰

```bash
# Worker Podì—ì„œ ì§ì ‘ ì‹¤í–‰
kubectl exec -n spark spark-worker-0 -- \
  spark-submit \
  --master spark://spark-master-svc:7077 \
  --deploy-mode client \
  --class org.apache.spark.examples.SparkPi \
  /opt/bitnami/spark/examples/jars/spark-examples_2.12-3.5.3.jar \
  100
```

**ê²°ê³¼ ì˜ˆì‹œ**:
```
Pi is roughly 3.140815914081591
Job finished: reduce at SparkPi.scala:38, took 16.696097 s
```

### ë©”íŠ¸ë¦­ ìƒì„± í™•ì¸

Job ì‹¤í–‰ í›„ Prometheusì—ì„œ ë©”íŠ¸ë¦­ í™•ì¸:

```bash
# Prometheusì—ì„œ ì¿¼ë¦¬
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090

# ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†: http://localhost:9090
# ì¿¼ë¦¬: metrics_master_apps_Value{namespace="spark"}
```

---

## 6. Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±

### ì ‘ì† ì •ë³´

```bash
# Grafana URL (Ingress ì‚¬ìš© ì‹œ)
http://grafana.k8s-cluster-01.miribit.lab

# Port-forwardë¡œ ë¡œì»¬ ì ‘ì†
kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80

# ê¸°ë³¸ ê³„ì •
Username: admin
Password: admin123
```

### DataOps ëŒ€ì‹œë³´ë“œì— Spark íŒ¨ë„ ì¶”ê°€

1. **Grafana ì ‘ì†** â†’ **Dashboards** â†’ **DataOps Overview**

2. **Add Panel** í´ë¦­

3. **Panel Title**: `Spark Cluster Workers`

4. **PromQL Query**:
```promql
metrics_master_aliveWorkers_Value{namespace="spark"}
```

5. **Visualization**: Stat

6. **Panel Options**:
   - Title: Spark Cluster Workers
   - Description: Number of alive Spark workers in the cluster
   - Unit: none
   - Decimals: 0

7. **Thresholds**:
   - Green: >= 2
   - Yellow: >= 1
   - Red: < 1

### ì¶”ê°€ ê¶Œì¥ íŒ¨ë„

#### íŒ¨ë„ 1: Spark Applications
```promql
metrics_master_apps_Value{namespace="spark"}
```
- Stat íŒ¨ë„
- í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜

#### íŒ¨ë„ 2: Spark Resource Usage
```promql
sum(rate(container_cpu_usage_seconds_total{namespace="spark"}[5m]))
```
- Graph íŒ¨ë„
- Spark í´ëŸ¬ìŠ¤í„° ì „ì²´ CPU ì‚¬ìš©ë¥ 

#### íŒ¨ë„ 3: Spark Memory
```promql
sum(container_memory_working_set_bytes{namespace="spark"}) / 1024 / 1024 / 1024
```
- Graph íŒ¨ë„
- Spark í´ëŸ¬ìŠ¤í„° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB)

---

## 7. ì•Œë¦¼ ê·œì¹™ (PrometheusRule)

### Spark Worker Down Alert

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: spark-alerts
  namespace: spark
  labels:
    release: kube-prometheus-stack
spec:
  groups:
    - name: spark
      interval: 30s
      rules:
        - alert: SparkWorkerDown
          expr: metrics_master_aliveWorkers_Value{namespace="spark"} < 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Spark cluster has no alive workers"
            description: "Spark cluster in namespace {{ $labels.namespace }} has {{ $value }} alive workers (expected >= 1)"

        - alert: SparkHighApplicationWaitQueue
          expr: metrics_master_waitingApps_Value{namespace="spark"} > 5
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Spark has high application wait queue"
            description: "Spark cluster has {{ $value }} applications waiting (threshold: 5)"
```

**ì ìš© ë°©ë²•**:
```bash
kubectl apply -f spark-alerts.yaml
```

---

## 8. ë©”íŠ¸ë¦­ ë³´ì¡´ ê¸°ê°„

### Prometheus ë¡œì»¬ ì €ì¥ì†Œ
- **ê¸°ë³¸ ë³´ì¡´**: 15ì¼
- **ìœ„ì¹˜**: Prometheus StatefulSet PVC

### Thanos ì¥ê¸° ì €ì¥ì†Œ
- **ë³´ì¡´ ê¸°ê°„**: S3 ì •ì±…ì— ë”°ë¦„ (ê¸°ë³¸ ë¬´ì œí•œ)
- **ë‹¤ìš´ìƒ˜í”Œë§**:
  - Raw: 7ì¼
  - 5m í•´ìƒë„: 30ì¼
  - 1h í•´ìƒë„: ì˜êµ¬ ë³´ì¡´

### ë©”íŠ¸ë¦­ ìš©ëŸ‰ ì¶”ì •

**Spark ë©”íŠ¸ë¦­ (ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°)**:
- ë©”íŠ¸ë¦­ ì¢…ë¥˜: ~50ê°œ
- ìƒ˜í”Œë§ ê°„ê²©: 30ì´ˆ
- ì¼ì¼ ë°ì´í„° í¬ì¸íŠ¸: 50 Ã— (86400 / 30) = 144,000
- ì˜ˆìƒ ì €ì¥ ìš©ëŸ‰: ~10MB/ì¼

**Kubernetes ë©”íŠ¸ë¦­ (Spark ë„¤ì„ìŠ¤í˜ì´ìŠ¤)**:
- Container ë©”íŠ¸ë¦­: ~20ê°œ Ã— 3 Pods = 60ê°œ
- Pod ë©”íŠ¸ë¦­: ~15ê°œ Ã— 3 Pods = 45ê°œ
- ì˜ˆìƒ ì €ì¥ ìš©ëŸ‰: ~5MB/ì¼

**ì´ ì˜ˆìƒ ìš©ëŸ‰**: ~15MB/ì¼ (Spark + Kubernetes)

---

## 9. ë¬¸ì œ í•´ê²°

### ë©”íŠ¸ë¦­ì´ ìˆ˜ì§‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°

1. **ServiceMonitor í™•ì¸**
```bash
kubectl get servicemonitor -n spark
kubectl describe servicemonitor -n spark spark-metrics
```

2. **Prometheus Target í™•ì¸**
```bash
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
# ë¸Œë¼ìš°ì €: http://localhost:9090/targets
# ê²€ìƒ‰: spark
```

3. **Spark ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ ì§ì ‘ í™•ì¸**
```bash
kubectl exec -n spark spark-master-0 -- \
  sh -c "apt-get update && apt-get install -y curl && curl http://localhost:8080/metrics/prometheus"
```

### Spark Jobì´ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ê²½ìš°

1. **Master ìƒíƒœ í™•ì¸**
```bash
kubectl logs -n spark spark-master-0
```

2. **Worker ì—°ê²° í™•ì¸**
```bash
kubectl logs -n spark spark-worker-0
```

3. **ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸**
```bash
kubectl exec -n spark spark-worker-0 -- \
  nc -zv spark-master-svc 7077
```

---

## 10. ì°¸ê³  ìë£Œ

### Spark ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ
- [Apache Spark Monitoring](https://spark.apache.org/docs/latest/monitoring.html)
- [Spark Metrics Configuration](https://spark.apache.org/docs/latest/configuration.html#metrics)

### Prometheus + Spark
- [Prometheus Servlet Sink](https://github.com/banzaicloud/spark-metrics)
- [Spark Exporter for Prometheus](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator)

### Grafana ëŒ€ì‹œë³´ë“œ
- [Community Spark Dashboard](https://grafana.com/grafana/dashboards/?search=spark)
- [DataOps Dashboard Examples](https://github.com/topics/dataops-dashboard)

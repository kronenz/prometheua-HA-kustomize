# ğŸŒ ë‹¨ì¼ 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„° Region ë¶„ë¦¬ ì „ëµ

> **í™˜ê²½**: 180ê°œ ë…¸ë“œê°€ ë‹¨ì¼ Kubernetes í´ëŸ¬ìŠ¤í„° + ëª¨ë‹ˆí„°ë§ ì „ìš© ë…¸ë“œ 7ëŒ€

## ğŸ“Š ë¬¸ì œ ì •ì˜

### ê¸°ì¡´ ì˜¤í•´
- âŒ 180ê°œ **í´ëŸ¬ìŠ¤í„°** (X)
- âœ… 180ê°œ **ë…¸ë“œ**ë¡œ êµ¬ì„±ëœ **ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°** (O)

### ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­

```yaml
êµ¬ì„±:
  - ë‹¨ì¼ Kubernetes í´ëŸ¬ìŠ¤í„°: 180ê°œ ì›Œì»¤ ë…¸ë“œ
  - ì¶”ê°€ ê·¸ë£¹:
    - Group B: 20ê°œ ë…¸ë“œ í´ëŸ¬ìŠ¤í„° (ë³„ë„)
    - Group C: 10ê°œ ë…¸ë“œ í´ëŸ¬ìŠ¤í„° (ë³„ë„)
    - Group D: 10ê°œ ë…¸ë“œ í´ëŸ¬ìŠ¤í„° (ë³„ë„)
  - ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„°: 7ê°œ ë…¸ë“œ

ëª©í‘œ:
  - 180ë…¸ë“œ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ Region ë¶„ë¦¬
  - ê° Regionë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì¡°íšŒ
  - Regionë³„ ë¶€í•˜ ë¶„ì‚°
```

---

## ğŸ¯ í•´ê²° ë°©ì•ˆ: ë…¼ë¦¬ì  Region ë¶„ë¦¬ (Node Labels + Relabeling)

### í•µì‹¬ ê°œë…

```mermaid
graph TB
    subgraph "Single 180-Node Cluster"
        direction TB

        subgraph "Region A1 (Node 1-60)"
            N1[Node 1-60<br/>label: region=a1]
            P1[Prometheus-A1<br/>scrape region=a1ë§Œ]
        end

        subgraph "Region A2 (Node 61-120)"
            N2[Node 61-120<br/>label: region=a2]
            P2[Prometheus-A2<br/>scrape region=a2ë§Œ]
        end

        subgraph "Region A3 (Node 121-180)"
            N3[Node 121-180<br/>label: region=a3]
            P3[Prometheus-A3<br/>scrape region=a3ë§Œ]
        end
    end

    subgraph "Monitoring Cluster (7 Nodes)"
        RQ1[Regional Query A1]
        RQ2[Regional Query A2]
        RQ3[Regional Query A3]
        GQ[Global Query]
    end

    P1 --> RQ1
    P2 --> RQ2
    P3 --> RQ3

    RQ1 --> GQ
    RQ2 --> GQ
    RQ3 --> GQ

    style N1 fill:#81c784
    style N2 fill:#4fc3f7
    style N3 fill:#ffb74d
    style GQ fill:#ff6b6b
```

---

## ğŸ—ï¸ ë‹¨ê³„ë³„ êµ¬í˜„ ì „ëµ

### Step 1: ë…¸ë“œ ë¼ë²¨ë§ (Region ë¶„ë¦¬)

#### 1.1 180ê°œ ë…¸ë“œ ë¼ë²¨ë§

```bash
# Region A1: Node 1-60
for i in {1..60}; do
  kubectl label node worker-node-$i region=a1 zone=zone-$((($i-1)/20 + 1))
done

# Region A2: Node 61-120
for i in {61..120}; do
  kubectl label node worker-node-$i region=a2 zone=zone-$((($i-61)/20 + 4))
done

# Region A3: Node 121-180
for i in {121..180}; do
  kubectl label node worker-node-$i region=a3 zone=zone-$((($i-121)/20 + 7))
done
```

**ë¼ë²¨ êµ¬ì¡°:**
```yaml
region: a1, a2, a3  # ëŒ€ë¶„ë¥˜ (60ê°œì”©)
zone: zone-1 ~ zone-9  # ì†Œë¶„ë¥˜ (20ê°œì”©, ì´ 9ê°œ zone)
```

#### 1.2 ë¼ë²¨ ê²€ì¦

```bash
# Regionë³„ ë…¸ë“œ ìˆ˜ í™•ì¸
kubectl get nodes -l region=a1 --no-headers | wc -l  # 60
kubectl get nodes -l region=a2 --no-headers | wc -l  # 60
kubectl get nodes -l region=a3 --no-headers | wc -l  # 60

# Zoneë³„ ë…¸ë“œ ìˆ˜ í™•ì¸
for zone in {1..9}; do
  echo "Zone $zone: $(kubectl get nodes -l zone=zone-$zone --no-headers | wc -l)"
done
```

---

### Step 2: Regionë³„ Prometheus ë°°í¬

#### 2.1 Prometheus Operator ì„¤ì •

```yaml
# deploy/single-cluster/prometheus-a1/prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus-a1
  namespace: monitoring
  labels:
    region: a1
spec:
  replicas: 2  # HA

  # Region A1 ë…¸ë“œì—ë§Œ ë°°í¬
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - a1
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              prometheus: prometheus-a1
          topologyKey: kubernetes.io/hostname

  # Region A1 ë©”íŠ¸ë¦­ë§Œ ìˆ˜ì§‘
  externalLabels:
    cluster: main-cluster
    region: a1
    prometheus_replica: "$(POD_NAME)"

  # Thanos Sidecar
  thanos:
    image: quay.io/thanos/thanos:v0.37.2
    objectStorageConfig:
      name: thanos-s3-config
      key: objstore.yml

  retention: 2h

  # ServiceMonitor ì„ íƒ (region=a1ë§Œ)
  serviceMonitorSelector:
    matchLabels:
      region: a1

  # PodMonitor ì„ íƒ (region=a1ë§Œ)
  podMonitorSelector:
    matchLabels:
      region: a1

  resources:
    requests:
      cpu: 2000m
      memory: 4Gi
    limits:
      cpu: 4000m
      memory: 8Gi
```

#### 2.2 ServiceMonitor í•„í„°ë§ (Relabeling)

```yaml
# deploy/single-cluster/prometheus-a1/servicemonitor-node-exporter.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-exporter-a1
  namespace: monitoring
  labels:
    region: a1  # Prometheusê°€ ì´ labelë¡œ ì„ íƒ
spec:
  selector:
    matchLabels:
      app: node-exporter

  endpoints:
  - port: metrics
    interval: 30s

    # Region A1 ë…¸ë“œë§Œ ìŠ¤í¬ë©
    relabelConfigs:
    # 1. Nodeì˜ region ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
    - sourceLabels: [__meta_kubernetes_node_label_region]
      action: keep
      regex: a1  # region=a1ì¸ ë…¸ë“œë§Œ ìœ ì§€

    # 2. Region ë¼ë²¨ ì¶”ê°€
    - sourceLabels: [__meta_kubernetes_node_label_region]
      targetLabel: region
      action: replace

    # 3. Zone ë¼ë²¨ ì¶”ê°€
    - sourceLabels: [__meta_kubernetes_node_label_zone]
      targetLabel: zone
      action: replace

    # 4. Node ì´ë¦„ ì¶”ê°€
    - sourceLabels: [__meta_kubernetes_node_name]
      targetLabel: node
      action: replace
```

#### 2.3 Prometheus A2, A3 ë™ì¼ êµ¬ì¡°ë¡œ ë°°í¬

```bash
# Prometheus A2 (Region A2)
sed 's/a1/a2/g' prometheus-a1.yaml > prometheus-a2.yaml
kubectl apply -f prometheus-a2.yaml

# Prometheus A3 (Region A3)
sed 's/a1/a3/g' prometheus-a1.yaml > prometheus-a3.yaml
kubectl apply -f prometheus-a3.yaml
```

---

### Step 3: ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Regional Query ë°°í¬

#### 3.1 Regional Query êµ¬ì„±

```yaml
# ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Node 3: Regional Query A1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: regional-query-a1
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: regional-query-a1
  template:
    metadata:
      labels:
        app: regional-query-a1
        region: a1
    spec:
      # ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Node 3ì— ë°°í¬
      nodeSelector:
        role: regional-a1

      containers:
      - name: thanos-query
        image: quay.io/thanos/thanos:v0.37.2
        args:
        - query
        - --http-address=0.0.0.0:9090
        - --grpc-address=0.0.0.0:10901
        - --query.replica-label=prometheus_replica

        # 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„°ì˜ Prometheus A1 Sidecar ì—°ê²°
        # Service Discovery ì‚¬ìš©
        - --store.sd-dns-interval=30s
        - --store=dnssrv+_grpc-sidecar._tcp.prometheus-a1-thanos-sidecar.monitoring.svc.cluster.local

        resources:
          limits:
            cpu: 4000m
            memory: 8Gi
          requests:
            cpu: 2000m
            memory: 4Gi
```

#### 3.2 Cross-Cluster Service Discovery

**ë¬¸ì œ**: ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„°ì—ì„œ 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„°ì˜ Prometheus Sidecarì— ì–´ë–»ê²Œ ì ‘ê·¼?

**í•´ê²°ì±… 1: LoadBalancer Service (ê¶Œì¥)**

```yaml
# 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë°°í¬
# Prometheus A1 Sidecarë¥¼ ì™¸ë¶€ë¡œ ë…¸ì¶œ
apiVersion: v1
kind: Service
metadata:
  name: prometheus-a1-sidecar-external
  namespace: monitoring
spec:
  type: LoadBalancer
  selector:
    prometheus: prometheus-a1
    thanos-store-api: "true"
  ports:
  - name: grpc
    port: 10901
    targetPort: 10901
  externalTrafficPolicy: Local
```

```yaml
# ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì—°ê²°
# Regional Query A1
args:
  - --store=<loadbalancer-ip-a1>:10901  # ì˜ˆ: 192.168.101.201:10901
```

**í•´ê²°ì±… 2: NodePort Service**

```yaml
# 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„°
apiVersion: v1
kind: Service
metadata:
  name: prometheus-a1-sidecar-external
  namespace: monitoring
spec:
  type: NodePort
  selector:
    prometheus: prometheus-a1
  ports:
  - name: grpc
    port: 10901
    targetPort: 10901
    nodePort: 30901  # ê³ ì • NodePort
```

```yaml
# ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„°
args:
  - --store=<any-worker-node-ip>:30901
```

**í•´ê²°ì±… 3: Ingress (gRPC)**

```yaml
# 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„°
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus-a1-sidecar-grpc
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
spec:
  ingressClassName: nginx
  rules:
  - host: prometheus-a1-sidecar.main-cluster.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus-a1-thanos-sidecar
            port:
              number: 10901
```

---

### Step 4: ì „ì²´ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "180-Node Cluster (Main Cluster)"
        direction TB

        subgraph "Region A1 (Node 1-60)"
            N1[Node 1-60<br/>region=a1]
            P1[Prometheus A1<br/>2 replicas]
            S1[Thanos Sidecar A1]

            N1 --> P1
            P1 --> S1
        end

        subgraph "Region A2 (Node 61-120)"
            N2[Node 61-120<br/>region=a2]
            P2[Prometheus A2<br/>2 replicas]
            S2[Thanos Sidecar A2]

            N2 --> P2
            P2 --> S2
        end

        subgraph "Region A3 (Node 121-180)"
            N3[Node 121-180<br/>region=a3]
            P3[Prometheus A3<br/>2 replicas]
            S3[Thanos Sidecar A3]

            N3 --> P3
            P3 --> S3
        end

        LB1[LoadBalancer<br/>192.168.101.201:10901]
        LB2[LoadBalancer<br/>192.168.101.202:10901]
        LB3[LoadBalancer<br/>192.168.101.203:10901]

        S1 --> LB1
        S2 --> LB2
        S3 --> LB3
    end

    subgraph "Monitoring Cluster (7 Nodes)"
        direction TB

        subgraph "Node 3"
            RQ1[Regional Query A1]
        end

        subgraph "Node 4"
            RQ2[Regional Query A2]
        end

        subgraph "Node 5"
            RQ3[Regional Query A3]
        end

        subgraph "Node 1-2"
            GQ[Global Query<br/>HA]
            G[Grafana]
        end

        subgraph "Node 7"
            S3_STORAGE[MinIO S3]
        end
    end

    LB1 -.->|gRPC| RQ1
    LB2 -.->|gRPC| RQ2
    LB3 -.->|gRPC| RQ3

    S1 -->|Upload| S3_STORAGE
    S2 -->|Upload| S3_STORAGE
    S3 -->|Upload| S3_STORAGE

    RQ1 --> GQ
    RQ2 --> GQ
    RQ3 --> GQ

    G --> GQ

    style N1 fill:#81c784
    style N2 fill:#4fc3f7
    style N3 fill:#ffb74d
    style GQ fill:#ff6b6b
    style S3_STORAGE fill:#90a4ae
```

---

## ğŸ“Š ë¦¬ì†ŒìŠ¤ ë°°ë¶„

### 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤

#### Prometheus ë°°í¬ (Regionë³„)

| Region | ë‹´ë‹¹ ë…¸ë“œ | Prometheus Replicas | CPU | Memory | ë°°í¬ ìœ„ì¹˜ |
|--------|----------|---------------------|-----|--------|----------|
| **A1** | 1-60 | 2 | 4 cores Ã— 2 = 8 cores | 8Gi Ã— 2 = 16Gi | Node 1-60 ì¤‘ ì„ íƒ |
| **A2** | 61-120 | 2 | 4 cores Ã— 2 = 8 cores | 8Gi Ã— 2 = 16Gi | Node 61-120 ì¤‘ ì„ íƒ |
| **A3** | 121-180 | 2 | 4 cores Ã— 2 = 8 cores | 8Gi Ã— 2 = 16Gi | Node 121-180 ì¤‘ ì„ íƒ |

**ì´ 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ ì˜¤ë²„í—¤ë“œ:**
- CPU: 24 cores (ì „ì²´ ë…¸ë“œì˜ ~0.01%)
- Memory: 48Gi (ì „ì²´ ë…¸ë“œì˜ ~0.01%)

### ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° (7 Nodes)

ì´ì „ ì„¤ê³„ì™€ ë™ì¼ ìœ ì§€

---

## ğŸ”§ ìƒì„¸ ë°°í¬ ê°€ì´ë“œ

### Phase 1: 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„° ì¤€ë¹„

#### 1.1 ë…¸ë“œ ë¼ë²¨ë§ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# label-nodes.sh

# Region A1 (1-60)
echo "Labeling Region A1..."
for i in {1..60}; do
  zone=$((($i-1)/20 + 1))
  kubectl label node worker-node-$i \
    region=a1 \
    zone=zone-$zone \
    monitoring-target=true \
    --overwrite

  if [ $((i % 10)) -eq 0 ]; then
    echo "  Labeled $i/60 nodes in Region A1"
  fi
done

# Region A2 (61-120)
echo "Labeling Region A2..."
for i in {61..120}; do
  zone=$((($i-61)/20 + 4))
  kubectl label node worker-node-$i \
    region=a2 \
    zone=zone-$zone \
    monitoring-target=true \
    --overwrite

  if [ $((i % 10)) -eq 0 ]; then
    echo "  Labeled $((i-60))/60 nodes in Region A2"
  fi
done

# Region A3 (121-180)
echo "Labeling Region A3..."
for i in {121..180}; do
  zone=$((($i-121)/20 + 7))
  kubectl label node worker-node-$i \
    region=a3 \
    zone=zone-$zone \
    monitoring-target=true \
    --overwrite

  if [ $((i % 10)) -eq 0 ]; then
    echo "  Labeled $((i-120))/60 nodes in Region A3"
  fi
done

# ê²€ì¦
echo ""
echo "=== Verification ==="
echo "Region A1: $(kubectl get nodes -l region=a1 --no-headers | wc -l) nodes"
echo "Region A2: $(kubectl get nodes -l region=a2 --no-headers | wc -l) nodes"
echo "Region A3: $(kubectl get nodes -l region=a3 --no-headers | wc -l) nodes"
echo ""
for zone in {1..9}; do
  count=$(kubectl get nodes -l zone=zone-$zone --no-headers | wc -l)
  echo "Zone $zone: $count nodes"
done
```

#### 1.2 ì‹¤í–‰

```bash
chmod +x label-nodes.sh
./label-nodes.sh
```

---

### Phase 2: Prometheus Operator ë°°í¬ (180ë…¸ë“œ í´ëŸ¬ìŠ¤í„°)

#### 2.1 kube-prometheus-stack ì„¤ì¹˜

```bash
# 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„°ì—ì„œ
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# ê¸°ë³¸ ì„¤ì¹˜ (Prometheus Operatorë§Œ)
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.enabled=false \
  --set alertmanager.enabled=false \
  --set grafana.enabled=false \
  --set prometheusOperator.enabled=true
```

#### 2.2 Prometheus A1 ë°°í¬

```yaml
# prometheus-a1.yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus-a1
  namespace: monitoring
spec:
  replicas: 2

  # Region A1 ë…¸ë“œì— ë°°í¬
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - a1

  externalLabels:
    cluster: main-cluster
    region: a1
    prometheus_replica: "$(POD_NAME)"

  # Thanos Sidecar
  thanos:
    image: quay.io/thanos/thanos:v0.37.2
    objectStorageConfig:
      name: thanos-s3-config
      key: objstore.yml
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 1000m
        memory: 2Gi

  retention: 2h

  # Region A1ë§Œ ìŠ¤í¬ë©
  serviceMonitorSelector:
    matchLabels:
      region: a1

  podMonitorSelector:
    matchLabels:
      region: a1

  resources:
    requests:
      cpu: 2000m
      memory: 4Gi
    limits:
      cpu: 4000m
      memory: 8Gi

  storageSpec:
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
```

```bash
kubectl apply -f prometheus-a1.yaml
```

#### 2.3 ServiceMonitor ìƒì„± (Node Exporter)

```yaml
# servicemonitor-node-exporter-a1.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-exporter-a1
  namespace: monitoring
  labels:
    region: a1
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: node-exporter

  endpoints:
  - port: metrics
    interval: 30s
    scheme: http

    relabelConfigs:
    # Region A1 ë…¸ë“œë§Œ ìŠ¤í¬ë©
    - sourceLabels: [__meta_kubernetes_endpoint_node_name]
      action: keep
      regex: worker-node-([1-9]|[1-5][0-9]|60)  # 1-60ë²ˆ ë…¸ë“œë§Œ

    # Region ë¼ë²¨ ì¶”ê°€
    - sourceLabels: [__meta_kubernetes_node_label_region]
      targetLabel: region
      action: replace

    # Zone ë¼ë²¨ ì¶”ê°€
    - sourceLabels: [__meta_kubernetes_node_label_zone]
      targetLabel: zone
      action: replace
```

**ë” ì •í™•í•œ í•„í„°ë§:**

```yaml
    relabelConfigs:
    # ë…¸ë“œì˜ region ë¼ë²¨ í™•ì¸
    - sourceLabels: [__meta_kubernetes_node_label_region]
      action: keep
      regex: a1  # region=a1ë§Œ ìœ ì§€
```

#### 2.4 Prometheus A2, A3 ë°°í¬

```bash
# A2 ìƒì„±
sed 's/a1/a2/g; s/1-60/61-120/g; s/worker-node-([1-9]|[1-5][0-9]|60)/worker-node-(6[1-9]|[7-9][0-9]|1[01][0-9]|120)/g' \
  prometheus-a1.yaml > prometheus-a2.yaml
kubectl apply -f prometheus-a2.yaml
kubectl apply -f servicemonitor-node-exporter-a2.yaml

# A3 ìƒì„±
sed 's/a1/a3/g; s/1-60/121-180/g; s/worker-node-([1-9]|[1-5][0-9]|60)/worker-node-(12[1-9]|1[3-7][0-9]|180)/g' \
  prometheus-a1.yaml > prometheus-a3.yaml
kubectl apply -f prometheus-a3.yaml
kubectl apply -f servicemonitor-node-exporter-a3.yaml
```

---

### Phase 3: Sidecar ì™¸ë¶€ ë…¸ì¶œ (LoadBalancer)

```yaml
# prometheus-a1-sidecar-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus-a1-sidecar-lb
  namespace: monitoring
  annotations:
    metallb.universe.tf/allow-shared-ip: "prometheus-sidecars"
spec:
  type: LoadBalancer
  loadBalancerIP: 192.168.101.201  # ì‚¬ì „ í• ë‹¹ëœ IP
  externalTrafficPolicy: Local
  selector:
    prometheus: prometheus-a1
  ports:
  - name: grpc
    port: 10901
    targetPort: 10901
    protocol: TCP
```

```bash
# 3ê°œ Region ëª¨ë‘ ìƒì„±
kubectl apply -f prometheus-a1-sidecar-lb.yaml
kubectl apply -f prometheus-a2-sidecar-lb.yaml  # IP: 192.168.101.202
kubectl apply -f prometheus-a3-sidecar-lb.yaml  # IP: 192.168.101.203

# ê²€ì¦
kubectl get svc -n monitoring | grep sidecar-lb
```

---

### Phase 4: ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Regional Query ì—°ê²°

```yaml
# ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Node 3
# regional-query-a1-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: regional-query-a1
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: regional-query-a1
  template:
    metadata:
      labels:
        app: regional-query-a1
    spec:
      nodeSelector:
        role: regional-a1  # ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Node 3

      containers:
      - name: thanos-query
        image: quay.io/thanos/thanos:v0.37.2
        args:
        - query
        - --http-address=0.0.0.0:9090
        - --grpc-address=0.0.0.0:10901
        - --query.replica-label=prometheus_replica

        # 180ë…¸ë“œ í´ëŸ¬ìŠ¤í„° Prometheus A1 Sidecar ì—°ê²°
        - --store=192.168.101.201:10901  # LoadBalancer IP

        resources:
          limits:
            cpu: 4000m
            memory: 8Gi
          requests:
            cpu: 2000m
            memory: 4Gi
```

---

## ğŸ” ê²€ì¦ ë° í…ŒìŠ¤íŠ¸

### 1. ë…¸ë“œ ë¼ë²¨ ê²€ì¦

```bash
# Regionë³„ ë…¸ë“œ ìˆ˜
kubectl get nodes -l region=a1 --no-headers | wc -l  # 60
kubectl get nodes -l region=a2 --no-headers | wc -l  # 60
kubectl get nodes -l region=a3 --no-headers | wc -l  # 60

# íŠ¹ì • ë…¸ë“œ ë¼ë²¨ í™•ì¸
kubectl get node worker-node-1 --show-labels | grep region
kubectl get node worker-node-61 --show-labels | grep region
kubectl get node worker-node-121 --show-labels | grep region
```

### 2. Prometheus íƒ€ê²Ÿ ê²€ì¦

```bash
# Prometheus A1 íƒ€ê²Ÿ í™•ì¸ (60ê°œ ë…¸ë“œë§Œ ìŠ¤í¬ë©í•´ì•¼ í•¨)
kubectl port-forward -n monitoring svc/prometheus-a1 9090:9090

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:9090/targets
# Filter: region="a1"
# ì˜ˆìƒ: 60ê°œ node-exporter íƒ€ê²Ÿ

# PromQLë¡œ í™•ì¸
curl -g 'http://localhost:9090/api/v1/query?query=up{region="a1"}' | jq '.data.result | length'
# ì˜ˆìƒ ì¶œë ¥: 60
```

### 3. Region ë¶„ë¦¬ ê²€ì¦

```promql
# Region A1 ë…¸ë“œ ìˆ˜
count(up{region="a1"})
# ì˜ˆìƒ: 60

# Region A2 ë…¸ë“œ ìˆ˜
count(up{region="a2"})
# ì˜ˆìƒ: 60

# Region A3 ë…¸ë“œ ìˆ˜
count(up{region="a3"})
# ì˜ˆìƒ: 60

# Regionë³„ CPU ì‚¬ìš©ë¥ 
sum(rate(node_cpu_seconds_total{mode!="idle"}[5m])) by (region)
```

### 4. Thanos Query ì—°ê²° ê²€ì¦

```bash
# ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Regional Query A1
kubectl exec -n monitoring regional-query-a1-xxx -- \
  wget -qO- http://localhost:9090/api/v1/stores | jq '.'

# ì˜ˆìƒ ì¶œë ¥:
# - 192.168.101.201:10901 (Prometheus A1 Sidecar)
# - labelSets: [{"region": "a1", "cluster": "main-cluster"}]
```

### 5. Global Query í†µí•© ê²€ì¦

```bash
# Global Queryì—ì„œ ì „ì²´ ì¡°íšŒ
kubectl exec -n monitoring global-thanos-query-xxx -- \
  wget -qO- 'http://localhost:9090/api/v1/query?query=count(up)' | jq '.'

# ì˜ˆìƒ: 180ê°œ ë…¸ë“œ ë©”íŠ¸ë¦­
```

---

## ğŸ“Š ì¶”ê°€ ê·¸ë£¹ í†µí•© (B, C, D)

### Group B (20ë…¸ë“œ), C (10ë…¸ë“œ), D (10ë…¸ë“œ) í´ëŸ¬ìŠ¤í„°

**ë™ì¼ ë°©ì‹ ì ìš©:**

```bash
# Group B í´ëŸ¬ìŠ¤í„° (ë³„ë„ Kubernetes í´ëŸ¬ìŠ¤í„°)
kubectl label node worker-node-{1..20} region=b zone=zone-b --context=cluster-b
kubectl apply -f prometheus-b.yaml --context=cluster-b

# Group C í´ëŸ¬ìŠ¤í„°
kubectl label node worker-node-{1..10} region=c zone=zone-c --context=cluster-c
kubectl apply -f prometheus-c.yaml --context=cluster-c

# Group D í´ëŸ¬ìŠ¤í„°
kubectl label node worker-node-{1..10} region=d zone=zone-d --context=cluster-d
kubectl apply -f prometheus-d.yaml --context=cluster-d
```

**ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Node 6ì—ì„œ í†µí•©:**

```yaml
# Regional Query BCD (ëª¨ë‹ˆí„°ë§ í´ëŸ¬ìŠ¤í„° Node 6)
args:
  - --store=192.168.101.204:10901  # Group B LB
  - --store=192.168.101.205:10901  # Group C LB
  - --store=192.168.101.206:10901  # Group D LB
```

---

## ğŸ¯ ìµœì¢… ì•„í‚¤í…ì²˜ ìš”ì•½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  180-Node Single Cluster                     â”‚
â”‚                                              â”‚
â”‚  Region A1 (Node 1-60)                       â”‚
â”‚    â†’ Prometheus A1 (2 replicas)             â”‚
â”‚    â†’ LoadBalancer: 192.168.101.201          â”‚
â”‚                                              â”‚
â”‚  Region A2 (Node 61-120)                     â”‚
â”‚    â†’ Prometheus A2 (2 replicas)             â”‚
â”‚    â†’ LoadBalancer: 192.168.101.202          â”‚
â”‚                                              â”‚
â”‚  Region A3 (Node 121-180)                    â”‚
â”‚    â†’ Prometheus A3 (2 replicas)             â”‚
â”‚    â†’ LoadBalancer: 192.168.101.203          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ gRPC (10901)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Monitoring Cluster (7 Nodes)               â”‚
â”‚                                              â”‚
â”‚  Node 3: Regional Query A1                  â”‚
â”‚  Node 4: Regional Query A2                  â”‚
â”‚  Node 5: Regional Query A3                  â”‚
â”‚  Node 6: Regional Query BCD                 â”‚
â”‚  Node 1-2: Global Query + Grafana           â”‚
â”‚  Node 7: MinIO S3                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [7_NODE_220_CLUSTER_ARCHITECTURE.md](./7_NODE_220_CLUSTER_ARCHITECTURE.md)
- [LARGE_SCALE_ARCHITECTURE.md](./LARGE_SCALE_ARCHITECTURE.md)
- Prometheus Operator: https://prometheus-operator.dev/
- Thanos Documentation: https://thanos.io/

---

**Last Updated**: 2025-10-15
**Architecture**: Single 180-Node Cluster with Logical Region Separation
**Key**: Node Labels + ServiceMonitor Relabeling + LoadBalancer

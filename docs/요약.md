아래는 “멀티 클러스터”에서 Prometheus를 모아 보려 할 때 흔히 쓰는 두 가지 패턴—① **Prometheus + Thanos Sidecar** vs ② **Prometheus Agent + Thanos Receiver(= Remote Write 수신) + Thanos(모듈)**—의 핵심 차이와 선택 가이드입니다.

# 1) 아키텍처 개요

```
[패턴 A] Sidecar 방식
각 클러스터
  Prometheus <-> Thanos Sidecar  --(TSDB 블록 업로드/메타 동기화)--> Object Storage
                                        \
                                         ---> Thanos Store GW
중앙
  Thanos Query(Frontend) <-> Store GW/Sidecar/Compactor/Rule
  Object Storage (S3/GCS/…)
```

```
[패턴 B] Agent + Receiver 방식
각 클러스터
  Prometheus Agent --(remote_write)--> Thanos Receiver(중앙/분산)
중앙
  Thanos Receiver(ingest) -> (WAL/TSDB 블록 생성) -> Object Storage
  Thanos Compactor/Store GW/Rule
  Thanos Query(Frontend)
```

---

# 2) 수집/저장/쿼리 흐름의 차이

|구분|Sidecar 패턴|Receiver + Agent 패턴|
|---|---|---|
|스크레이프|**풀 Prometheus**(스크레이프+로컬 TSDB+룰)|**Prometheus Agent**(스크레이프만, 로컬 TSDB 미보관)|
|전송|사이드카가 **로컬 TSDB 블록을 주기적 업로드**(Ship) + 메타 노출|Agent가 **remote_write로 스트림 전송**|
|장기 저장|Object Storage에 **각 Prometheus 인스턴스가 직접 블록 업로드**|**Receiver가 중앙에서 블록 생성** 후 Object Storage에 업로드|
|쿼리|Thanos Query가 **StoreGW/Sidecar**를 통해 오브젝트스토리지+실시간 데이터 읽음|Thanos Query가 **StoreGW(리시버/오브젝트스토리지)**를 통해 읽음|
|실시간성|로컬 Prometheus가 최신 시계열 보유 → 쿼리 지연 적음|최신 데이터는 Receiver/ingester 경유. 구성에 따라 지연 소폭 증가 가능|
|룰/알람|각 클러스터 Prometheus나 Thanos Ruler 둘 다 가능|**Thanos Ruler**(중앙) 사용 권장(Agent는 룰 미지원)|

---

# 3) 운영 특성(스케일/복원/비용)

**스케일링**

- **Sidecar**: 클러스터 수↑ = Prometheus(풀) 수↑. 각자 TSDB 관리 및 업로드. StoreGW/Query 쪽 메타 개수 증가.
- **Receiver+Agent**: **수집면은 경량(Agent)**, **중앙 Receiver가 수평 확장**(해시링/리플리카)으로 대규모 멀티클러스터/멀티테넌시 대응 유리.

**복원력/장애 도메인**

- **Sidecar**: 개별 Prometheus 노드 장애 시 그 노드의 최신 데이터에만 영향. 로컬 리텐션이 있어 네트워크 단절 시에도 **버퍼링**(나중 업로드) 가능.
- **Receiver+Agent**: 네트워크 단절 시 Agent의 remote_write 큐로 **단기 버퍼링** 가능하나 긴 단절엔 취약. 반면 중앙 Receiver가 이중화/샤딩되어 있으면 **수집 경로의 HA**가 더 간명.

**비용/자원**

- **Sidecar**: 각 클러스터에 **풀 Prometheus(메모리/디스크)** 필요. 로컬 TSDB 디스크 비용↑. 업로드 시 네트워크 egress 비용.
- **Receiver+Agent**: 에지(클러스터)에서 **디스크·메모리↓(Agent)**. 대신 **중앙 Receiver 집합**의 스토리지/네트워크/CPU 비용이 커짐.

**운영 난이도**

- **Sidecar**: 구성 단순(클러스터마다 Prometheus+Sidecar). 오브젝트스토리지 권한을 에지에 배포해야 함.
- **Receiver+Agent**: 중앙에 Receiver 링/테넌시/리텐션/라벨링 정책 설계 필요. 에지는 간단, 중앙은 복잡.

---

# 4) 기능적 차이(룰, 테넌시, 보안, 백필)

- **룰/알람**
    
    - Sidecar: 각 클러스터 Prometheus Rule + 중앙 Thanos Ruler 혼용 쉬움(이중화/분산 알람).
    - Receiver+Agent: 일반적으로 **Thanos Ruler 중앙집중**으로 관리(클러스터별 룰 분산 관리 필요 없을 때 강점).
- **멀티테넌시/격리**
    
    - Sidecar: 테넌시 모델은 주로 **라벨링/쿼리 RBAC**로 처리.
    - Receiver+Agent: Receiver에서 **테넌시 헤더/라우팅**으로 **수집 단계부터 격리**가 명확(엔터프라이즈/호스팅 환경에 유리).
- **백필/재처리**
    
    - Sidecar: 각 노드가 블록을 업로드하므로 백필은 개별 TSDB 단위 조정.
    - Receiver+Agent: 중앙에서 블록이 만들어져 **컴팩션/리텐션 정책을 한 곳**에서 운용, 백필/리텐션 관리가 일원화.
- **보안/네트워크**
    
    - Sidecar: 에지에서 오브젝트스토리지 접근키 필요(권한 범위 최소화 주의). 업로드 트래픽은 배치성.
    - Receiver+Agent: 에지는 **remote_write만 아웃바운드**. 오브젝트스토리지 키는 **중앙에만** 두면 됨.

---

# 5) 지연/정합성/데이터 손실 관점

- **지연(latency)**
    
    - Sidecar: 로컬 TSDB가 있어 **초근접 실시간**. 업로드 지연은 장기보관에만 영향.
    - Receiver+Agent: remote_write → Receiver 경유. 튜닝 전제 시 실시간성 충분하지만, **추가 홉**으로 수백 ms~수 s 지연 가능.
- **정합성/중복**
    
    - Sidecar: 각 인스턴스가 자체 블록 업로드. 리라벨·샤딩이 어긋나면 중복 시계열 가능.
    - Receiver+Agent: Receiver가 **리플리카 합치기(HA dedup)** 및 **해시링**으로 **중복·분산 제어**를 중앙에서 일괄 처리하기 쉬움.
- **데이터 손실**
    
    - Sidecar: 네트워크 단절 시에도 로컬 TSDB에 계속 적재 → 복구 후 업로드, **손실 위험 낮음**.
    - Receiver+Agent: Agent 큐 용량 초과/장기 단절 시 **드롭 가능성**(큐/재시도 파라미터로 완화).

---

# 6) 언제 무엇을 고를까? (현실적인 가이드)

**Sidecar가 적합**

- 클러스터 수가 **소~중간 규모**(수~수십).
- **현장(에지)에도 디스크 여유** 있고, 네트워크가 가끔 불안정(버퍼링 중요).
- 각 클러스터별 **자율 룰/알람** 운영(로컬 즉시 알람)이 필요.
- **간단하고 검증된 패턴**을 원할 때.

**Receiver + Agent가 적합**

- **대규모 멀티클러스터/멀티테넌시**(수십~수백) 운영.
- 에지 리소스를 **최소화**하고, **중앙에서 정책·리텐션·보안**을 일괄 관리하고 싶을 때.
- **오브젝트스토리지 자격증명**을 에지에 배포하기 싫을 때.
- **중앙 집중형 룰/알람(Thanos Ruler)** 선호, 테넌트별 격리/과금/스로틀 등 플랫폼형 운영.

---

# 7) 실무 팁(양쪽 공통/차별 포인트)

- **레이블 설계**: `cluster`, `namespace`, `tenant` 등 필수 라벨을 초기에 표준화. Receiver 방식은 **테넌트 헤더 ↔ 라벨** 매핑 전략을 문서화.
- **압축/리텐션**: Thanos Compactor 파라미터와 Object Storage 라이프사이클(예: S3 IA/Glacier)로 비용 최적화.
- **HA/중복 제거**: Prometheus 리플리카(`external_labels`로 구분)와 Thanos의 **deduplication** 옵션 조합 확인.
- **네트워크**: remote_write(Receiver) 경로는 **TLS/인증** 필수. Sidecar 업로드는 **VPC 엔드포인트/프록시** 고려.
- **알람 지연**: 중앙 Ruler만 쓸 때 네트워크/쿼리 지연이 SLA에 맞는지 확인. 로컬 알람이 필요하면 Sidecar/풀 Prometheus.

---
좋아요 👍  
아래는 앞서 설명한 내용을 **표 + 시각 구조(도식)**로 보기 쉽게 정리한 문서 버전입니다.

---

# 🧭 멀티 클러스터 모니터링 아키텍처 비교

**Prometheus + Thanos Sidecar vs Prometheus Agent + Thanos Receiver**

---

## 1️⃣ 전체 구조 개념도

```
① Sidecar 방식
 ┌────────────────────────────┐
 │ Cluster A                  │
 │  ┌────────────┐            │
 │  │Prometheus  │────┐       │
 │  └────────────┘    │TSDB   │
 │       ▲             ▼       │
 │  ┌────────────┐     │       │
 │  │Thanos Sidecar│───┘────▶ Object Storage
 │  └────────────┘             │
 └────────────────────────────┘
             │
             ▼
        Thanos Query / StoreGW / Compactor / Ruler
```

```
② Agent + Receiver 방식
 ┌────────────────────────────┐
 │ Cluster A                  │
 │  ┌────────────────────────┐│
 │  │Prometheus Agent        ││
 │  │ (scrape + remote_write)││
 │  └────────────────────────┘│
 └──────────────┬─────────────┘
                │
                ▼
        ┌────────────────────┐
        │ Thanos Receiver(s) │───▶ Object Storage
        └────────────────────┘
                │
                ▼
     Thanos Query / StoreGW / Compactor / Ruler
```

---

## 2️⃣ 비교 표

|구분|**Sidecar 방식**|**Receiver + Agent 방식**|
|---|---|---|
|**구성요소**|Prometheus(풀 기능) + Thanos Sidecar|Prometheus Agent + Thanos Receiver|
|**데이터 전송**|로컬 TSDB 블록을 주기적 업로드(Ship)|remote_write 스트림으로 실시간 전송|
|**데이터 저장 위치**|각 Prometheus가 Object Storage에 직접 업로드|중앙 Receiver가 블록 생성 후 Object Storage에 저장|
|**실시간성**|로컬 TSDB로 최신 데이터 즉시 쿼리 가능|중앙 Receiver를 거쳐 약간의 지연 발생|
|**장애 내성**|네트워크 단절 시에도 로컬 버퍼링 → 재업로드 가능|Agent의 큐 한도 내 버퍼링 가능 (장기 단절엔 손실 위험)|
|**운영 규모**|소~중규모 클러스터에 적합|대규모, 멀티테넌시 환경에 유리|
|**알람/룰**|로컬 Prometheus 또는 Thanos Ruler 혼용|중앙 Thanos Ruler 권장 (Agent는 룰 미지원)|
|**리소스 요구**|각 클러스터별 디스크, CPU, 메모리 필요|중앙 Receiver 쪽 부하 집중, 에지는 경량|
|**보안/권한**|에지 Prometheus에 Object Storage 접근키 필요|오브젝트스토리지 접근은 중앙 Receiver만 수행|
|**복구/백필**|개별 TSDB 업로드 재처리|중앙에서 일괄 관리 용이|
|**운영 난이도**|구조 단순, 관리 분산|중앙 집중형 설계 필요, 복잡도↑|
|**멀티테넌시 격리**|라벨 기반|Receiver 테넌시 헤더 기반 (격리 명확)|
|**적합 환경**|네트워크 불안정, 단순 구성, 자율 알람 중요|대규모, 정책/보안 일원화, SaaS형 모니터링|

---

## 3️⃣ 장단점 요약 (아이콘 시각)

||**Sidecar** 🧩|**Receiver + Agent** 🛰️|
|:--|:--|:--|
|✅ **장점**|• 구성 간단  <br>• 네트워크 단절에도 안정  <br>• 로컬 알람 가능|• 중앙 집중 관리  <br>• 경량 Agent로 리소스 절약  <br>• 멀티테넌시/보안 격리 우수|
|⚠️ **단점**|• 각 클러스터 디스크/자원 부담  <br>• 오브젝트스토리지 권한 분산|• 중앙 Receiver 복잡  <br>• 긴 네트워크 단절 시 손실 위험|
|🏁 **적합 규모**|소~~중규모 (10~~20 클러스터)|대규모 (수십~수백 클러스터)|

---

## 4️⃣ 선택 가이드

|조건|추천 구성|
|---|---|
|클러스터 수가 적고, 현장 자율 모니터링 필요|**Prometheus + Thanos Sidecar**|
|클러스터 수가 많고, 중앙집중형 운영/보안 중요|**Prometheus Agent + Thanos Receiver**|
|네트워크 단절 가능성 있음|**Sidecar (로컬 버퍼링)**|
|클라우드 SaaS나 대규모 멀티테넌시 운영|**Receiver + Agent**|

---

## 5️⃣ 요약 도식

```
규모/운영 복잡도 ↑
   │
   │                 Receiver+Agent
   │                     ▲
   │                     │  대규모, 중앙집중형, 멀티테넌시
   │                     │
   │         Sidecar      │
   └──────────────────────┘
        소규모, 단순, 에지 자율형
```

---

